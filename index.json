[{"content":"联仁健康-如何用大模型实现电子病历数据后治理 医学临床数据治理的难点： 不同医院、科室甚至医生的记录风格和用词可能不同 文本信息之间可能存在复杂的关联性和依赖性 同一个描述可能在不同上下文中有不同的含义。 一方面，病历信息的语义环境复杂且结构细碎，呈现异构性和难互通特征，使得数据的完整性、一致性和准确性都难以保证；另一方面，由于标准化认知和遵循不足，医学术语标准化程度不高，导致大量信息资料无法检索，统计结果缺乏普遍性和客观性。\n电子病历数据的规模化、复杂化、多样性、动态性、非标化都是医学临床数据治理的难点。\n针对电子病历数据，联仁健康引入大模型技术，从自然语言结构化，医学术语标准化，数据质量规范化三个方面，降低成本，落实规范，稳定质量。 大模型结构化电子病历文本 第一步对大段文本进行信息提取，包括从入院记录、出院小结、手术记录、病程记录等提取医生需要的临床指标数据。 为了缓解幻觉，采用了多种干预策略： 模型侧使用对齐调整策略，多种提示词优化入参 基于业务逻辑对输出结果进行校验 利用大模型标准医学术语 为了满足医学术语标准化，用医学词汇表和标准化组织的术语和定义，如医学主题词汇（MeSH）、国际疾病分类 (ICD)、国家医保和国家药监发布的药品、诊疗项目、耗材目录等，保证医学数据的标准，使得更加利于统计分析。\n大模型本身在计算资源不足的情况下不适用术语标准化，但可以利用其总结和摘要能力，让其输出和医生表达方式一致的回答，再将输出给联仁自研的术语标准化算法，可以得到更加精准的医学术语归一化结果。\n通常，术语标准化采用的技术方案框架为：多路召唤+精排\n联仁使用的方案是：实体识别+向量编码+快速检索，并用模型量化加速技术，适配不同业务场景。\n例如：椎间盘突出-\u0026gt;椎间盘突出症，焦虑抑郁状态-\u0026gt;焦虑性神经症。\n构建标准化字典（如疾病编码 ICD-10 以及药品 ACT 编码等），将不规范的规范\n数据处理，标准化部分可以看看，主要是搞质控的流程\n森亿智能大模型医疗应用 能够根据患者的信息，跟随医生思路自动灵活扩写病历，包括病情描述、鉴别诊断、治疗方案等信息，边写边生成，无需选择病历模板即可完成病历生成。包括不同角色（交班、汇报、出院）多种角度和方式总结生成病历摘要、治疗建议，出院小结。\n懂医生思维，智能扩写文本，提质增效。例如医生提供”主诉“，根据信息自动生成”起病情况表述“，当医生补充肿块边缘清晰”这一描述信息后，系统自动补全了“肿块质地、活动性\u0026quot;等同类伴随症状的描述。 多样化总结病历摘要，根据不同临床应用场景，以多种方式对文书进行总结。 智能生成出院小结，将手术记录的手术经过，概括写进出院小结的诊疗经过。 通过诊疗经过，生成出院记录。\n阮彤：大模型给电子病历互联互通带来的变革 — 从结构互通到语义互通 不同医疗机构之间的电子病历互联互通，可以相互处理。\n大模型用于电子病历的数据治理 大模型用于电子病历的数据治理\n电子病历数据来自 EMR 数据库，基于电子病历提取慢病患者的关键信息，如患病时长、症状时长、治疗方式。\nEMR\n大量字段/变量未标化 含有大量非结构化数据 常存在矛盾数据 RCD\n尽可能实现字段/变量标准化 核心/关键文本信息结构化 包含人口学特征（基础信息）、暴露、结局等信息 数据治理通常包含： 异常数据处理（缺失、矛盾） 关键信息提取 标准化（手术、诊断、器械、科室） 问题： 重要信息未提取\n如一诉六史中，患者出现症状的时间，既往患病，既往用过哪些药等，在病历中未结构化。 关键信息命名多样化\n各个医院对于同个诊断/手术，可能有不同的描述。\n图1：同一诊断不同描述图 病历质量参差不齐 电子病历关键信息提取 电子病历重要信息包含在“一诉六史”中，以既往史为例，从既往史中抽取14个类型的实体及其之间的关系。 实体类型：疫苗名称, 手术名称, 时长, 疾病名称, 治疗方式, 过敏源, 传染病名称, 否定词, 输血史, 时间, 原因, 外伤部位, 治疗结果, 输血原因 ；关系类型：时长, 治疗方式, 有关, 否定词, 时间, 原因, 治疗结果, 输血原因。\n图2：UIE和ERNIE提取实体流程 关键信息的归一 采用大小模型组合编排的方式实现，大模型采用了私有化部署的医学垂直大模型，此外还是用阿里的RANER做关键词提取，以消除异常数据。\n图3：关键信息归一图 病历书写相关流程 患者办理住院后，住院医师在患者床旁进行病史采集以及体格检查，同时会在便签本或者提前打印的问诊模板上记录病史的要点，然后回到办公室，在院内系统的电脑上进行电子病历的书写。 阮彤：ChatGPT等大语言模型对医疗信息系统的影响与提升——大语言模型在医疗行业应用系列（一） 语言模型会推动人机交互方式的变化，代码与文档的自动生成、数据的自动分析处理\n比较重要的变迁可能会在三大类系统上，自然语言信息记录与检索类系统，临床知识库与临床科研系统。\n图4：变迁图 自然语言信息记录与检索类系统 大语言模型的特点在于强大的自然语言理解与生成能力。该能力衍生出了不同格式文档的理解与生成，数据格式的语义转换，数据的自动化的清洗等等。更重要的是，这些能力的激发，并不需要标注大量语料，Zero-Shot、Few-Shot InContext learning以及一些简单的自然语言提示(prompt)，就可以达到比较好的效果。\n图5：CBLUE测评效果 电子病历系统的核心功能是可定制的EMR模板，以及基于模板的引导式数据录入。难点并不在于定制模板，而是模板定义规范很难确立，对模板的扩充限制，会影响模板的灵活性，但如果不限制，又会影响模板的利用。而有了大模型，可以放入大量的经典例子进行指令微调，然后通过使用简单的提示(prompt)，表达模板的含义，生成相应的病历数据。而电子病历的质控，也完全可以通过prompt定义质量要求，再自动生成，或是修改过程，或是输入完成后，进行质量控制。其他功能，如导入检验检查数据，或是文档的修改，都可以通过自然语言交互接口完成。\n知识库与临床决策支持类系统 具体而言，可以通过chatGPT的开源框架langchain的一系列工具，以及chatGPT的各种Zero-Shot抽取能力，将原始书本数据抽取成更为合适的形式。而对于临床决策类的知识库应用，在提升知识库对嵌套、条件、分支和异常、概率等各种表达能力基础上，综合更多的真实世界数据。可以将病人病情通过大模型进行解析，在知识库上进行推理。也可以将知识自动生成数据，与真实数据一起，放入大模型进行训练。当然，大模型的可解释性、不稳定性以及缺乏严密的逻辑推理等问题，也会给临床决策支持带来障碍，可以通过分解决策的步骤，通过外接知识库，对推理的每一步进行严格溯源与控制得到缓解。\n阮彤：医疗系统获得大语言模型能力的途径——大语言模型在医疗行业应用系列（二） 应用尝试顺序与方案 1.知识库与文献检索系统 使用ChatGPT+LangChain等。由于不涉及内部数据，可以基于现有ChatGPT的LangChain框架以及相关应用直接完成。首先汇集自己常用的医学书籍和文献。其次抽取或解析这些文件，并使用向量库方式，对文档进行语义检索。进一步的，可以根据知识库目标，构造问题与多个问题之间的关联，抽取相关书籍和文献，然后组织成较为结构化的形式，供相关用户浏览。\n2.电子病历检索系统 使用开源模型+微调。根据前期作者在部分电子病历上的测试结果，ChatGPT语言模型对电子病历的敏感度是相当高的，只是数据可能不能放到外网。预计基于开源语言模型的搜索，也会比现有的关键词搜索效果要好。如果不能获得很好的效果，进行一些持续训练或者预训练，应该能取得比较好的效果。\n3.RDR/区域卫生平台/专病库 采用开源模型+微调/复杂Prompt。这些系统的核心除了检索之外，会有较多的数据清洗与不同类型的数据转换。这个过程一步完成效果可能不会太好。预计需要多个步骤完成，而步骤的分解，以及每个Prompt怎么写，需要进一步实验。\n4.电子病历录入 采用开源模型+微调/复杂Prompt。有了很好的语义检索和数据转换功能，电子病历录入就可以集中在内容的撰写，而不是格式的要求上。基于Prompt生成会有幻觉现象，另外，电子病历语言有自己的风格，不同专科电子病历也有特定的要求，如何将这种要求同时让医生和机器了解，需要挑选更为经典的用例，更多的微调以及比较好的Prompt设计。\n5.临床科研 采用ChatGPT/开源模型+类AutoGPT框架。这个方向前期在于问题解析和调研，中期在于获取数据以及数据分析，后期在于基于数据的论文撰写。任务有较大的复杂度和各种整合要求，但由于本身对精准度要求并不高，同时也可以有用户的交互，因此准入门槛并不高。这个工作早期可以也通过ChatGPT的各种软件拼接完成。\n6.决策支持系统 采用开源模型+微调+外接知识库+推理+图像OCR解析+长上下文。如前文所说，决策系统类型丰富，涉及到的技术点较多，可能还会补充各种检查报告结果，对于普通机构来说，可以在特定决策应用上，比如，预问诊、体检报告阅读，或是特定专科上进行尝试。\n与上述任务相关的，是上述任务效果的评估。目前电子病历检索、数据清洗与转换、电子病历录入、决策支持等，都缺乏合适的判断标准，以及相应的标准数据集合进行测试。特别的，对于电子病历录入，还需要通过大规模的RLHF进行提升。而对于决策支持，需要提高决策过程的可解释性，才能真正的让医生和病人信服。\n医疗大模型从小事做起（一）：患者群管理 医患交流沟通时，有的临床科室会建立微信群，但是随着群越来越多，渐渐管不过来。通过接入一个 bot 解决管理问题。可以向用户提供信息检索、资料推送等服务。提供群聊要点自动整理，并生成待办、预约等服务。\n但是这种交互有一个特点，只有用户@bot时，才会触发对话，所以其实也就是一个单用户聊天 bot\n期望的 bot 应该能解决三个问题：\n听的懂，能从多用户、多轮次、多线程讨论中，准确地领悟不同主线地主旨、关键词等重要信息 接得住，说话不僵硬，不突兀 学得会，可以从群里地真人医患对话中，不断学习行业知识和语料，持续提示自己。 文章提到微软的一篇论文：https://www.chima.org.cn/Sites/Uploaded/File/2024/04/20240425001.pdf 介绍了一种基于大模型的群组聊天机器人框架，从“何时“、”向谁“、”说什么“三个设计维度，为多用户聊天机器人给出完整的设计思路。\n患者随访可以接入，科室患者群，在随访维系关系时，有重要作用，接入大模型可以帮助医生护士减轻回答压力。\n曹剑峰：AIGC与医疗展望 通用信息抽取技术UIE(Universal Information Extraction)，结合医学NLP提供通用的后结构化增益：基于Prompt思想，将希望抽取的Schema信息转换成“线索词”(Schema-based Prompt)作为模型输入的前缀，使得模型理论上能够适应不同领域和任务的Schema信息，并按需抽取出“线索词”指向的结果，从而实现开放域环境下的通用信息抽取。\n医疗领域直接的受益点就是未来患者的“电子病历”和“电子健康档案”中的主要医疗文书，将通过大模型的泛在语义结合指令微调小模型的精练语义相融合，而自动生成部分核心内容，其内容也将是文字结合图片和视频甚至部分3D内容。\n以前述的医疗文书为例，文档的模板更规范、质控逻辑更清晰、内容表达更丰富、易读性和可解释性也更强。而且可以极大地把医务人员从日常繁琐的文件梳理工作中解放出来，把更多的精力投入到患者医疗救治服务和临床科研中去。\n可实现个性化内容服务，聊天机器人和“数字人”成为新的、更包容性的用户交互界面。医疗可对接的场景很容易想到未来“12320”热线的无人值守、数字家医智能患者随访、健康评估、健康咨询与宣教、远程医疗等需要大量人机交互的场景和医疗服务新模式领域。\n黄昊：电子病历的临床思维初探 老教授的声音传来，“我们的病历质量是越来越差了，大段的复制粘贴，一线医生根本不会写病历，也不认真观察病人，这样下去要不得。”“电子病历对于临床来说，就是倒退。”\n临床医生现在有大量病历要写，已经被限制在电脑旁，根本没有时间去床旁观察病情;有的说，我们病历输入方式太原始，在其他医院早就采用语音录入了，方便又快捷;有的还说，门诊病人入院，我们根本看不到门诊各项检查资料\u0026hellip;\u0026hellip;\n老主任有些不好意思地说：“我这不是针对你，但是现在的电子病历对临床思维的培训没有一点好处，医生们都不会写病历了。你作为信息科主任，要重视。”\n主任说：“病历是临床思维呈现的客观载体，作为临床诊疗的记录，是整个临床资料从收集到汇总再到病情分析的全过程。通过书写病历，医生思考、分析患者的病情，找出病因，对症治疗。” “但是现在的电子病历，让这个过程消失了，医生面对的是各种模板，删删改改就完成了记录，甚至粗心大意的人会出现很多低级错误，临床医生的基本功严重退化。”\n老教授痛心疾首的话语，勾起了我的探究欲望，到底什么是临床思维，我们的信息系统又该如何去建设才更能将临床思维体现出来?\n通过文献查阅，我发现，这种担忧已经成为了全行业的普遍共识，即住院电子病历系统不能充分体现住院医师的临床思维及决策能力。\n医学大家张孝骞教授曾经对临床思维做了如下定义：临床思维是对疾病现象进行调查、分析、综合、判断、推理等一系列的思维活动，以认识疾病的本质。它既是重要的诊断方法，也适用于疾病的治疗。\n核心问题是，患者如何诊断和治疗?因此临床思维又划分为诊断思维和治疗思维。\n病历书写是临床思维、技能操作、人文理念在文字上的体现。医生从具体病例入手，发现问题、提出问题、分析问题、解决问题，从而进一步提升临床思维和综合应用能力。这是医学院校对病历书写的要求。\n结合规定和临床需求，医生在写病历时，需要将所涉及的信息进行汇总，在此基础上归纳、综合，丰富的数据可以帮助建立清晰的临床诊断思路，对理解病例资料及做出诊断具有重要价值。通过归纳或综合的逻辑学思维，把临床资料转化为可以进行判断推理的形式。描述这些结果说明了什么问题，或代表什么意义，而不是罗列具体检查结果。\n要说软件开发工程师是很懂逻辑的，信息系统就是建立在一系列逻辑判断基础上的代码集。但是这些逻辑和临床医生治病救人的思维逻辑还有很大区别。电子病历现在已经打通了各业务系统，为临床提供了完整且丰富的各类信息，已经实现了数据的归集汇总功能。但是分析和总结还是由医生完成，大量数据之间缺乏关联性校验，病历的内涵质量提升尚需要各级医生去检查和督促等等，这些都让临床医生满是抱怨。现有的电子病历系统在功能设计上还是有很多不足，其实质是人脑和电脑在表达上还存在巨大差异。虽然说，当今计算机科学已经取得了巨大进步，但是在思维模式上，计算机更强调通用性，人脑更注重个性表达，这种差异短时间难以拟合。\n具体到病历书写环节，电子病历要求能够较好满足管理和数据分析统计要求，却难以很好适应个性化要求;手写病历将不同医生的特点完美呈现，但是在数据共享和利用上却受限。随着人工智能技术发展，未来电脑会更理解人脑思维，那时的系统也就更能贴合医生的工作习惯，电子病历也许会将原有纸质病历的优点呈现。\n建议：\n1.系统应该分用户进行数据呈现和提醒。 在如今的互联网时代，用户的信息化体验早已经被技术推进到新的高度，以用户为主的web2.0技术早已广泛应用，程序呈现的功能和界面是根据不同用户的需求和习惯所决定的。在临床来说，常规我们按照住院医师、主治医师、主任(副主任)医师三级检诊负责制运行。住院医师负责日常病历书写，电子病历系统除了承担记录和信息汇总角色外，还应该发挥分析和校验的功能，帮助住院医师全面了解病情，提升其临床思维能力;主治医师负责质量管理、病情的决策和临床带教，电子病历系统就应该是其忠实的数字助理，提醒重要事项，避免遗漏和差错发生;主任医师对于电子病历的使用，更多是检查督查、疑难病历决断、会诊、科研数据收集，电子病历就应按此需求进行功能呈现。只有满足了不同用户的需求，才能得到更多认同。\n2.系统需以临床视角重构数据采集与共享，完善病历书写逻辑性，使之更符合临床诊断和治疗的特点。 “在汇总临床资料基础上，运用逻辑思维与系统思维，强化归纳与综合能力，应用排除诊断法或综合诊断法思考可能的诊断并进行鉴别，充分把握一元论、常见病和多发病优先、器质性疾病优先、可治性疾病优先的原则。除主要疾病诊断以外，还需注意伴发病、合并症、并发症的诊断，注意现象和本质、主要表现和次要表现、共性与个性、局部与整体的关系”。(摘自图书《临床思维》，姚树坤、张杼扬主编)这段话我原样照搬，需要IT人员细品，感悟其中真味，产品自然就上档次了。\n现如今，实施电子病历不只是信息科主导的信息化项目，而更应该是医务部门主导的医疗质量提升的信息化项目。在建设中充分体现医务部门的作用，将各科室发动起来，发挥好病历模板的作用，平衡好公共模板和患者病情个性分析这两者之间的关系，通过标准模板减轻住院医师工作量，同时通过适当控制，引导住院医师去分析资料、分析病情，培养临床思维。只有大家共同参与和配合，才能做出一套适合临床需要的病历系统。\n3.系统可以嵌入通用人工智能技术，实现逻辑推理及判断。 以ChatGPT为代表的通用大模型的出现，加剧了临床对电子病历的新期待。通过不断对话，模型就可以理解用户的需求，进而为其提供满足需求的作品。这个模式让医生产生了强烈共鸣，因为医疗问诊的过程就是这样。大家自然就会想，什么时候电子病历可以根据我们录入的各项数据，给出一份满足管理、医疗、医保等各种要求的合格病历。这也给产品开发商提供了一个新产品方向，也只有充分应用人工智能技术，电子病历才会更像一个具备临床思维能力的好工具。通用人工智能技术当然是未来发展的方向，也许还需要几年的技术沉淀和积累，目前也可以把对病历数据的利用工作做起来，利用数据来分析住院医师书写病历中的问题，帮助医师调整和优化临床思维。\n文章提出一个期望，电子病历可以根据录入的各项数据给出一份合格病历，将问诊过程，检查等各项数据输入即可生成相应的合格病历。\n同时提到临床思维核心是判断患者如何诊断和治疗，即分为诊断思维和治疗思维。\n而大模型擅长逻辑推理和判断，需要将大模型融合到临床思维中，帮助医生在完善电子病历的同时，给出一定的辅助诊断和治疗建议。\n完善电子病历 在一诉五史中，首先根据规范明确这一诉五史需要包含哪些描述，将每一部分的内容结构化 将其结构化以后，将数据中的相应内容也进行结构化，对应相同的结构将不同描述但相同意思的部分进行转化成规范且统一的描述；对于患者提到的与规范不同意义的部分，将其转化成其特殊的表述并嵌入进统一的规范表达中。\neg：既往史规范为：”无过敏史，无手术史，无。。。“，若患者有过敏经历，在输入既往史的过程中只需要输入”过敏源为 xxx“，将生成：”有过敏史，过敏源：xxx“。\n指令数据应该这样构造，对电子病历中每一个自然语言构成的部分，理解其含义，并为了减轻医生输入负担，从前是选择模板，找到其相应的内容在进行修改，转变为，医生直接在病历的某个部分对相应病情内容进行描述，即可自动生成该部分嵌入相应修改之后的内容。 临床思维逻辑的辅助 诊断思维逻辑 在通过病历记录的相应的内容之后，结合一诉五史，体格检查，辅助检查，可以相应的比较完整地掌握了患者的病情信息，这时候可以通过逻辑判断，推断其相应的诊断的概率。\n也就是 task1 的任务目的所在。\n治疗思维逻辑 治疗分为：药物治疗、放射治疗、物理治疗、心理治疗等。\n这部分内容在病程记录中具有部分内容，但不完整，可以初步作为一个治疗的思维的判断，同时对于具体某种病的治疗，可以根据相应的教科书案例来判断诊疗。\n这部分内容作为 task2。\n从诊室到云端：医疗大模型的应用挑战与未来探索 从诊室到云端：医疗大模型的应用挑战与未来探索\n医疗大模型的逻辑框架 数据层：电子病历，文献报告，医学知识图谱，医学图像，同时进行数据清洗、标注、统一编码 模型层：使用 transformer 框架，输入大规模医疗预料，通过 masked LM、Next sentence prediction 等方式进行无监督预训练。 应用层：预训练模型微调、结合医学知识图谱、规则库等知识源增强医学专业性，使用知识蒸馏、参数剪枝等技术压缩模型并在真实临床环境中评估、调优。经过验证的模型部署到医疗信息系统、移动设备等，提供智能服务。 医疗大模型的主要应用场景和适用范围 在医疗实践场景中，大模型可以协助分析临床文本以提取关键信息，从而加快医生的诊断和治疗建议。\n在患者护理和保健过程中，医疗大模型可助力实现以下几方面工作：一是远程监测患者健康，特别是慢性病患者，分析生理参数、设备数据和健康记录，帮助医生管理疾病，减少住院和急诊；二是分析健康记录、生活方式和基因信息，识别风险因素和早期疾病迹象，实现个体化健康管理和疾病预防；三是减轻家庭照护负担，提供远程医疗建议和护理指导，改善远程护理体验。如，讯飞医疗诊后康复管理平台基于星火认知大模型，专注于康复指导和诊后管理，提供个性化康复计划，服务延伸至患者日常生活。\n在医院管理流程中，医疗大模型可实现以下功能：一是优化患者流程管理，根据患者需求和临床优先级合理分配医院资源，涵盖排队、资源分配和病房管理，提高患者满意度和医疗效率；二是分析患者反馈、社交媒体评论和医疗调查数据，发掘患者意见和需求，为医疗机构提供建议，助力医疗服务改进和患者体验提升。如，东软针对医疗领域推出添翼大模型，全方位融合医疗行业解决方案、产品与服务，添翼的多模态数据融合能力可为医院管理者提供对话式交互和数据洞察，简化数据应用，实现精细化医院管理。\n在药物研发过程中，医疗大模型可预测药物-蛋白质相互作用和药物毒性等信息，从而评估新药的功效和安全性，有助于缩减研发周期，加速新药发现。如，清华系初创团队水木分子推出新一代对话式药物研发助手ChatDD，涵盖药物立项、临床前研究、临床试验各阶段，作为制药专家的AI助手，提升药物研发人员的工作效率。\n基于AI的预问诊，患者在候诊的阶段，就可以收集病人的病情、症状、过往用药等相关信息，事先生成结构化的病历。不仅大大提升线上诊疗的效率，改善了就医秩序，也提高了患者的就医满意度。\n巨头们涌入的医疗大模型，何时迎来最好的商业时代？ 幻觉发生怎么解决 我们采用多种手段从多方面降低幻觉，包括保证医疗预训练语料和微调数据的质量和多样性、采用能降低知识幻觉的解码策略、融合医疗知识图谱的知识增强大模型技术、医疗知识检索增强、大模型结果后校验、大模型输出置信度评估等。\n哪一个标准最能代表医疗大模型的水平\n临床有效性是最能代表医疗大模型水平的关键评价标准，包括模型在实际临床环境中的诊断准确性、治疗建议的合理性以及与专业医生的决策一致性。此外，模型的鲁棒性、泛化能力、可解释性、用户友好性、数据隐私保护以及合规性也是重要的评价维度。然而，临床有效性直接关系到患者的安全和健康，因此如果把医疗大模型应用与临床实践中，它可能是最重要的评价标准。 如何让大众认可大模型的诊断或治疗方案？\n第一，要提高模型的决策过程透明度，提供可解释的输出，让用户理解模型是如何得出结论的。这有助于建立用户信任，尤其是对于医疗决策这样敏感的问题。第二，要有严格的临床试验，证明模型的诊断或治疗方案与专业医生的判断相当或更优，且这些结果应由独立的第三方机构审核并公开。第三，要让医生参与到模型的开发和应用中，他们可以提供专业知识，确保模型的输出符合医学实践，并在实际应用中监督和调整。第四，要开展公众教育活动，解释人工智能在医疗领域的潜力和限制，消除误解，提高公众的理解和接受度。 通过这些措施，应该可以逐步提高社会对大模型在医疗领域应用的接受度和信任度。 云知声的山海医疗大模型主要做了哪些场景？目前哪个场景的应用率最高？哪个场景能算作山海的“杀手锏”？\n对于云知声的山海医疗大模型，主要做了以下场景： 病历生成：包括基于医患对话的门诊病历和出院小结、手术记录生成等住院病历的生成，以及放射科报告生成等医技科报告。 病历质控：对住院病历（包括病案首页）做过程和终末质控，支持 1000+形式和内涵质控点，大幅提高病历的质量。 单病种上报：对国家卫健委要求的 57 个病种做自动数据汇集及上报。 医保控费：按照医保局的规范，监管医院的临床诊疗行为和收费合理性，确保医疗费用的合规。 保险理赔的医疗审核：审核在保险理赔中涉及到的医疗费用，剔除不合理费用。 专病库平台：将病历等临床数据自动抽取和导入到专病库。 智能问诊：作为 AI 医生，与患者进行对话，收集症状，并提供初步的健康咨询和建议。 目前，山海应用率最高的场景是病历生成、病历质控和保险理赔的医疗审核。结合云知声在语音技术上强项开发出的门诊病历生成系统，结合云知声在医疗知识图谱的积累开发的病历质控系统和保险理赔医疗审核系统均可以视为“杀手锏”场景。\n人工智能大模型赋能医疗健康白皮书 架构\n图6：架构图 商量大医模型\n图7：商量大医模型架构 在辅助诊疗和临床决策领域的应用\n图8：医疗大模型应用辅助诊疗和临床决策领域 助力临床文档生成和医疗文本结构化\n图9：医疗文本结构化 人工智能大模型在医疗领域的应用现状与前景展望 图10：架构构想 Truveta Language Model unlocks EHR data for the most complete and accurate medical research Healthcare data is recorded in heterogeneous systems with millions of different ways clinicians, hospitals, and health systems express observations, diagnoses, medication plans, and more. Clinicians use different terms based on their location, training, and expertise. “Acute COVID-19,” “COVID,” “COVID-19,” “COVID infection,” and “COVID19 _ acute infection” (and hundreds of other variations) all refer to COVID-19, and “600mg Ibuprofen” and “Ibuprofen 600mg” are the same thing. To analyze healthcare data, this diverse medical language, including misspellings or abbreviations, must be normalized to medical information ontologies (e.g., LOINC for lab tests, GUDID for medical devices, etc.).\n**AI models are only as good as the data they are trained upon. ** TLM is trained upon data from Truveta’s health system members currently representing more than 80 million patient journeys, including 5.5 billion diagnoses, 3.1 billion encounters, and 2.4 billion medication orders. Updated daily, Truveta Data combines this EHR data with social drivers of health (SDOH) data, claims, and mortality for unmatched breadth and depth of data for research. Using this unprecedented data, Truveta’s clinical expert annotators label tens of thousands of raw clinical terms to train TLM to normalize healthcare data for clinical accuracy, and then check the results of the model as it runs.\nClinician notes hold critical information about the patient journey, such as disease stages, adverse events, medication change rationales, and disease symptoms not found in claims data sets, nor found in most structured EHR analytics data. For example, a structured dataset might include a medication and later a diagnosis of a rash, but the clinician note is the only place where those two concepts are connected, showing the rash as an adverse reaction to the medication.\n要分析医疗保健数据，必须根据医疗信息本体论（如实验室检验的 LOINC、医疗设备的 GUDID 等）对这些不同的医疗语言（包括拼写错误或缩写）进行规范化。\nTLM 使用了8000 多万患者的就医过程，包括 55 亿诊断、31 亿就诊和 24 亿用药指令。\n临床医生笔记中包含有关患者病程的关键信息，如疾病阶段、不良事件、换药理由和疾病症状等，这些信息在理赔数据集中找不到，在大多数结构化电子病历分析数据中也找不到。例如，一个结构化数据集可能包括一种药物和随后的皮疹诊断，但临床医生笔记是将这两个概念联系起来的唯一地方，它将皮疹显示为药物的不良反应。\n问诊内容 问诊的内容包括什么？\n（一）一般情况\n包括：姓名、性别、年龄籍贯、出生地、民族、婚姻、住址、工作单位、职业、人院日期、记录日期、病史陈述者及可靠程度等。若病史陈述者并非本人，则应注明其与病人的关系。记录年龄时应填写实足年龄，不可以“儿”或“成”代替，因年龄本身亦具有诊断参考意义。\n（二）主诉\n病人感受最主要的疾苦或最明显的症状和体征，即就诊最主要的原因。主诉应言简意明，用一、两句话全面概括，并注明疾病发生到就诊的时间。\n（三）现病史\n病史中的主体部分，包括疾病的发生、发展及演变的全过程，是问诊中的重点内容。主要包括以下几个方面：\n1.起病情况（缓急）与患病的时间。\n2.主要症状的特点，包括所在的部位、放射区域、性质、发作频度、持续时间、强度、加重或缓解的因素。\n3.发作原因与诱因。\n4.病情的发展与演变（按时间顺序记录，包括主要症状的发展和其他有关症状的情况）。\n5.伴随症状。\n6.诊断、治疗经过（药物、剂量、疗效等）。\n7.患病以来的一般情况（精神状态、食欲、体重改变、睡眠及大小便等情况）。\n8.归纳、小结，再度核实。\n9.用过渡语言转入过去史的问诊。\n（四）既往史\n又称“过去史”。包括：\n1.病人既往的健康状况。\n2.过去曾患过的疾病（包括各种传染病），特别是与现病有密切关系的疾病史。如冠状动脉粥样硬化性心脏病患者，应询问过去是否有过高血压病、糖尿病等。记述时应注意不要和现病史混淆。\n3.外伤、手术、意外事故和预防接种史。\n4.过敏史（对药物、食物及环境因素）。\n5.对居住或生活地区的主要传染病和地方病，也应记录于既往史中。\n6.记录顺序一般按年、月的先后排列。\n（五）个人史\n与健康和疾病有关的个人经历。包括：\n1.社会经历包括出生地、居住地区和居留时间（尤其是疫源地和地方病流行区）、受教育程度、经济生活和业余爱好等。\n2.职业及工作条件包括工种、劳动环境、对工业毒物的接触情况及时间。\n3.习惯与嗜好起居与卫生习惯、饮食的规律与质量，烟酒嗜好与摄入量等。\n4.冶游史有无不洁性交，是否患过淋病、尖锐湿疣、下疳等。\n（六）家族史\n指病人家族中有关成员的健康状况等，包括：\n1.双亲的年龄及健康情况（儿科包括祖父母、外祖父母）。\n2.配偶的年龄和健康情况。\n3.兄弟、姐妹的年龄和健康情况。\n4.子女的年龄及健康情况。\n5.家族中有无与患者同样的疾病，有无与遗传有关的疾病，如白化病、血友病、先天性球形细胞增多症、糖尿病、家族性甲状腺功能减退症、精神病等。对已死亡的直系亲属要问明死因与年龄。医学教育|网搜集整理有些遗传性疾病的家族史中还应包括某些非直系亲属。\n","permalink":"https://tanxiangyuu.github.io/posts/llm/%E5%8C%BB%E7%96%97%E6%96%87%E6%9C%AC%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%91%E5%B1%95/","summary":"联仁健康-如何用大模型实现电子病历数据后治理 医学临床数据治理的难点： 不同医院、科室甚至医生的记录风格和用词可能不同 文本信息之间可能存在复杂的关联性和依赖性 同一个描述可能在不同上下文中有不同的含义。 一方面，病历信息的语义环境复杂且结构细碎，呈现异构性和难互通特征，使得数据的完整性、一致性和准确性都难以保证；另一方面，由于标准化认知和遵循不足，医学术语标准化程度不高，导致大量信息资料无法检索，统计结果缺乏普遍性和客观性。\n电子病历数据的规模化、复杂化、多样性、动态性、非标化都是医学临床数据治理的难点。\n针对电子病历数据，联仁健康引入大模型技术，从自然语言结构化，医学术语标准化，数据质量规范化三个方面，降低成本，落实规范，稳定质量。 大模型结构化电子病历文本 第一步对大段文本进行信息提取，包括从入院记录、出院小结、手术记录、病程记录等提取医生需要的临床指标数据。 为了缓解幻觉，采用了多种干预策略： 模型侧使用对齐调整策略，多种提示词优化入参 基于业务逻辑对输出结果进行校验 利用大模型标准医学术语 为了满足医学术语标准化，用医学词汇表和标准化组织的术语和定义，如医学主题词汇（MeSH）、国际疾病分类 (ICD)、国家医保和国家药监发布的药品、诊疗项目、耗材目录等，保证医学数据的标准，使得更加利于统计分析。\n大模型本身在计算资源不足的情况下不适用术语标准化，但可以利用其总结和摘要能力，让其输出和医生表达方式一致的回答，再将输出给联仁自研的术语标准化算法，可以得到更加精准的医学术语归一化结果。\n通常，术语标准化采用的技术方案框架为：多路召唤+精排\n联仁使用的方案是：实体识别+向量编码+快速检索，并用模型量化加速技术，适配不同业务场景。\n例如：椎间盘突出-\u0026gt;椎间盘突出症，焦虑抑郁状态-\u0026gt;焦虑性神经症。\n构建标准化字典（如疾病编码 ICD-10 以及药品 ACT 编码等），将不规范的规范\n数据处理，标准化部分可以看看，主要是搞质控的流程\n森亿智能大模型医疗应用 能够根据患者的信息，跟随医生思路自动灵活扩写病历，包括病情描述、鉴别诊断、治疗方案等信息，边写边生成，无需选择病历模板即可完成病历生成。包括不同角色（交班、汇报、出院）多种角度和方式总结生成病历摘要、治疗建议，出院小结。\n懂医生思维，智能扩写文本，提质增效。例如医生提供”主诉“，根据信息自动生成”起病情况表述“，当医生补充肿块边缘清晰”这一描述信息后，系统自动补全了“肿块质地、活动性\u0026quot;等同类伴随症状的描述。 多样化总结病历摘要，根据不同临床应用场景，以多种方式对文书进行总结。 智能生成出院小结，将手术记录的手术经过，概括写进出院小结的诊疗经过。 通过诊疗经过，生成出院记录。\n阮彤：大模型给电子病历互联互通带来的变革 — 从结构互通到语义互通 不同医疗机构之间的电子病历互联互通，可以相互处理。\n大模型用于电子病历的数据治理 大模型用于电子病历的数据治理\n电子病历数据来自 EMR 数据库，基于电子病历提取慢病患者的关键信息，如患病时长、症状时长、治疗方式。\nEMR\n大量字段/变量未标化 含有大量非结构化数据 常存在矛盾数据 RCD\n尽可能实现字段/变量标准化 核心/关键文本信息结构化 包含人口学特征（基础信息）、暴露、结局等信息 数据治理通常包含： 异常数据处理（缺失、矛盾） 关键信息提取 标准化（手术、诊断、器械、科室） 问题： 重要信息未提取\n如一诉六史中，患者出现症状的时间，既往患病，既往用过哪些药等，在病历中未结构化。 关键信息命名多样化\n各个医院对于同个诊断/手术，可能有不同的描述。\n图1：同一诊断不同描述图 病历质量参差不齐 电子病历关键信息提取 电子病历重要信息包含在“一诉六史”中，以既往史为例，从既往史中抽取14个类型的实体及其之间的关系。 实体类型：疫苗名称, 手术名称, 时长, 疾病名称, 治疗方式, 过敏源, 传染病名称, 否定词, 输血史, 时间, 原因, 外伤部位, 治疗结果, 输血原因 ；关系类型：时长, 治疗方式, 有关, 否定词, 时间, 原因, 治疗结果, 输血原因。","title":"医疗文本大模型发展"},{"content":"研究目标 该研究设计和实现了一个能实现多项任务和应用的医学大模型，ClinicalGPT。通过在训练过程中引入多种真实世界数据（如医疗记录、领域特定知识以及多轮对话咨询等）的方式，使其更好地应对多个临床任务。此外，作者还提出了一种全面的评估框架，包括医学知识问答、医学考试、患者咨询以及对电子病历进行诊断分析等多个方面。\n实际问题：\n传统的自然语言处理方法往往需要手动设计特征或规则来进行分类或回答问题，这种方法不仅费时费力，而且难以适应不同场景的需求。而大规模语言模型可以通过无监督学习的方式自动学习到大量的语言规律和语义信息，因此具有更好的泛化能力和适应性。然而，大模型在医疗领域的实际使用中具有局限性。这些模型产生的结果往往与事实不符、逻辑不一致、不连贯，例如引用不存在的文章。它们缺乏真实世界的推理技能和基础，导致了通用和含糊的回复。具体而言，ChatGPT 在医学背景下缺乏深度和洞察力，这可能是由于其基于奖励的训练所使用的排列模型，该模型生成的答案过于笼统，缺乏医学专业知识。\n因此，该研究提出的知识图谱模板化技术和奖励模型可以帮助语言模型更好地理解和应对医学领域中的挑战，从而实现更高效的任务完成。\n是否是新问题：\n虽然在医疗大模型所面临的挑战并非全新，但本研究采用的方法\u0026ndash;开发像 ClinicalGPT 这样的专用模型并针对医疗任务对其进行评估\u0026ndash;却是新颖的。本研究试图直接解决这些已知问题，重点是提高模型在医疗应用中的性能。\n对产业发展的重要意义：\n这项研究对医疗行业的意义在于，它有可能打破特定任务的范式，实现人工智能在医疗保健领域的多功能应用\n新思路与方法 思路\n该研究使用了多种医疗数据集进行模型训练和评估，包括cMedQA2、cMedQA-KG、MD-EHR、MEDQA-MCMLE和MedDialog等。其中：\ncMedQA2是一个包含12万条问题和22.6万条答案的中文医学问答数据集； cMedQA-KG是基于知识图谱构建的医学问答数据集，包含了丰富的医学实体及其关系； MEDQA-MCMLE是基于中国医学考试题库构建的数据集，用于评估临床推理能力； MedDialog是一个由在线平台收集的多轮医学对话数据集，可以模拟医生与患者之间的交互过程； MD-EHR则是一个来自多个大型医院的电子病历数据集，涵盖了各种疾病类型。 为了增强语言模型在特定领域的应用效果，该研究采用了知识图谱模板化技术将知识图谱中的实体和关系转化为适合于指令微调的文本对形式，并利用监督微调的方式进一步优化模型性能。\n此外，该研究还引入了一种奖励模型来为强化学习提供反馈信号，以提高模型生成高质量且有用的输出的概率。\n解决方案关键\n多样的训练数据集 通过知识图谱模板化技术将知识图谱中的实体和关系转化为文本对的形式，可以更好地利用这些结构化的信息来指导模型的学习过程。 同时，奖励模型可以为强化学习提供有效的反馈信号，从而帮助模型生成更加准确和有用的回答。 知识图谱模板化技术如下表所示：\nPrompt Response {疾病}和哪些疾病有关联？ {疾病}可能与{疾病}有关联。 {疾病}可能与其他哪些疾病有关？ {疾病}可能与{疾病}有关联。 {疾病}有哪些常见症状？ {疾病}的常见症状包括{临床表现}。 患有{疾病}的患者可能出现哪些症状？ {疾病}患者可能出现如{临床表现}等症状。 {疾病}的典型临床表现是什么？ {疾病}的典型临床表现为{临床表现}。 患有{疾病}的患者在临床上通常表现为哪些症状？ 患有{疾病}的患者在临床上通常表现为{临床表现}。 诊断{疾病}需要进行哪些检查？ 诊断{疾病}需要进行如{医学检验项目}等检查。 如何检查以确定患有{疾病}？ 确定患有{疾病}需要进行{医学检验项目}等检查。 {药物}主要用于治疗哪些疾病？ {药物}主要用于治疗{疾病}等疾病。 {药物}的适应症是什么？ {药物}的适应症为治疗{疾病}。 如何治疗{疾病}？ 治疗{疾病}的方法包括{医疗程序}。 {疾病}的常见治疗方法有哪些？ {疾病}的常见治疗方法包括{医疗程序}。 {疾病}会引起哪些并发症？ {疾病}会引起{疾病}等并发症。 患有{疾病}的患者可能出现哪些并发症？ 患有{疾病}的患者可能出现{疾病}等并发症。 {药物}与哪些药物存在相互作用？ {药物}与{药物}存在相互作用。 使用{药物}时需要注意哪些药物相互作用？ 使用{药物}时需注意与{药物}的相互作用。 {药物}主要用于治疗哪些症状？ {药物}主要用于治疗{临床表现}等症状。 {药物}的主要治疗作用是什么？ {药物}的主要治疗作用为治疗{临床表现}。 实验设计 本文涉及了以下四个方面的实验：\n医疗对话实验：作者采用了BLEU、ROUGE和GLEU三个评价指标来评估模型生成的对话质量。实验结果表明，ClinicalGPT在所有ROUGE指标上表现优秀，在BLEU-1和大部分ROUGE指标上也表现出色。 医学考试实验：作者选择了MEDQA-MCMLE数据集中的几个类别进行评估，包括医学伦理、呼吸系统、消化系统等。实验结果显示，ClinicalGPT在各个类别中均表现出色，超过了其他LLMs的平均准确率。 诊断实验：作者使用MD-EHR测试集对模型的诊断能力进行了评估。实验结果表明，ClinicalGPT在各个疾病组别中都表现出色，尤其是在消化系统和泌尿系统方面表现最佳。 医学问答实验：作者使用cMedQA2数据集对模型的问答能力进行了评估。实验结果表明，ClinicalGPT在与BLOOM-7B、LLAMA-7B和ChatGLM-6B的比较中表现最好，尤其在与BLOOM-7B和LLAMA-7B的比较中胜出较多。 总的来说，本文的实验结果表明，ClinicalGPT在医疗领域具有出色的应用潜力，特别是在理解和生成医疗对话、医学检查和诊断等方面。然而，在某些特定领域，如妇科和血液系统，可能需要进一步改进。\n论文贡献 多样性数据集构建:训练数据使用了不同来源，覆盖全面的医疗数据\n评估框架构建:建立了包括问答、考试、咨询以及对诊断分析等不同方面的评估标准\n知识图谱模板化:将知识图谱知识转换为 promp 形式，提供更利于理解的训练方式。\n值得探索的问题与挑战 虽然ClinicalGPT在医疗领域的表现已经非常出色，但仍然存在一些挑战需要克服。\n模型准确性提升：如何确保模型输出的准确性、可解释性和安全处理敏感健康数据等问题。因此，在未来的研究中，可以考虑开发更加精细的评估指标和更有效的模型调整方法，以进一步提高模型的性能和适用性。\n医疗场景不足：可以探索将模型应用于更多的医疗场景，如临床决策支持、临床试验招募、临床数据管理和研究支持等领域，为医疗行业带来更大的价值。\n不足和缺失 ClinicalGPT 虽然效果相对其他消费型大模型效果较好，但缺少与闭源模型的比较，并且相关效果并没有达到特别理想的状态。\n学到的内容与启发 可以利用开源数据构建不同场景下的医疗指令，并且使用不同格式的数据源，将其转成可用于微调的指令格式。 数据来源要具备一定的多样性 医疗场景构建也要丰富，意味着指令可以更加丰富，应用可以更加多。 ","permalink":"https://tanxiangyuu.github.io/posts/paper_read/cinicalgpt/","summary":"研究目标 该研究设计和实现了一个能实现多项任务和应用的医学大模型，ClinicalGPT。通过在训练过程中引入多种真实世界数据（如医疗记录、领域特定知识以及多轮对话咨询等）的方式，使其更好地应对多个临床任务。此外，作者还提出了一种全面的评估框架，包括医学知识问答、医学考试、患者咨询以及对电子病历进行诊断分析等多个方面。\n实际问题：\n传统的自然语言处理方法往往需要手动设计特征或规则来进行分类或回答问题，这种方法不仅费时费力，而且难以适应不同场景的需求。而大规模语言模型可以通过无监督学习的方式自动学习到大量的语言规律和语义信息，因此具有更好的泛化能力和适应性。然而，大模型在医疗领域的实际使用中具有局限性。这些模型产生的结果往往与事实不符、逻辑不一致、不连贯，例如引用不存在的文章。它们缺乏真实世界的推理技能和基础，导致了通用和含糊的回复。具体而言，ChatGPT 在医学背景下缺乏深度和洞察力，这可能是由于其基于奖励的训练所使用的排列模型，该模型生成的答案过于笼统，缺乏医学专业知识。\n因此，该研究提出的知识图谱模板化技术和奖励模型可以帮助语言模型更好地理解和应对医学领域中的挑战，从而实现更高效的任务完成。\n是否是新问题：\n虽然在医疗大模型所面临的挑战并非全新，但本研究采用的方法\u0026ndash;开发像 ClinicalGPT 这样的专用模型并针对医疗任务对其进行评估\u0026ndash;却是新颖的。本研究试图直接解决这些已知问题，重点是提高模型在医疗应用中的性能。\n对产业发展的重要意义：\n这项研究对医疗行业的意义在于，它有可能打破特定任务的范式，实现人工智能在医疗保健领域的多功能应用\n新思路与方法 思路\n该研究使用了多种医疗数据集进行模型训练和评估，包括cMedQA2、cMedQA-KG、MD-EHR、MEDQA-MCMLE和MedDialog等。其中：\ncMedQA2是一个包含12万条问题和22.6万条答案的中文医学问答数据集； cMedQA-KG是基于知识图谱构建的医学问答数据集，包含了丰富的医学实体及其关系； MEDQA-MCMLE是基于中国医学考试题库构建的数据集，用于评估临床推理能力； MedDialog是一个由在线平台收集的多轮医学对话数据集，可以模拟医生与患者之间的交互过程； MD-EHR则是一个来自多个大型医院的电子病历数据集，涵盖了各种疾病类型。 为了增强语言模型在特定领域的应用效果，该研究采用了知识图谱模板化技术将知识图谱中的实体和关系转化为适合于指令微调的文本对形式，并利用监督微调的方式进一步优化模型性能。\n此外，该研究还引入了一种奖励模型来为强化学习提供反馈信号，以提高模型生成高质量且有用的输出的概率。\n解决方案关键\n多样的训练数据集 通过知识图谱模板化技术将知识图谱中的实体和关系转化为文本对的形式，可以更好地利用这些结构化的信息来指导模型的学习过程。 同时，奖励模型可以为强化学习提供有效的反馈信号，从而帮助模型生成更加准确和有用的回答。 知识图谱模板化技术如下表所示：\nPrompt Response {疾病}和哪些疾病有关联？ {疾病}可能与{疾病}有关联。 {疾病}可能与其他哪些疾病有关？ {疾病}可能与{疾病}有关联。 {疾病}有哪些常见症状？ {疾病}的常见症状包括{临床表现}。 患有{疾病}的患者可能出现哪些症状？ {疾病}患者可能出现如{临床表现}等症状。 {疾病}的典型临床表现是什么？ {疾病}的典型临床表现为{临床表现}。 患有{疾病}的患者在临床上通常表现为哪些症状？ 患有{疾病}的患者在临床上通常表现为{临床表现}。 诊断{疾病}需要进行哪些检查？ 诊断{疾病}需要进行如{医学检验项目}等检查。 如何检查以确定患有{疾病}？ 确定患有{疾病}需要进行{医学检验项目}等检查。 {药物}主要用于治疗哪些疾病？ {药物}主要用于治疗{疾病}等疾病。 {药物}的适应症是什么？ {药物}的适应症为治疗{疾病}。 如何治疗{疾病}？ 治疗{疾病}的方法包括{医疗程序}。 {疾病}的常见治疗方法有哪些？ {疾病}的常见治疗方法包括{医疗程序}。 {疾病}会引起哪些并发症？ {疾病}会引起{疾病}等并发症。 患有{疾病}的患者可能出现哪些并发症？ 患有{疾病}的患者可能出现{疾病}等并发症。 {药物}与哪些药物存在相互作用？ {药物}与{药物}存在相互作用。 使用{药物}时需要注意哪些药物相互作用？ 使用{药物}时需注意与{药物}的相互作用。 {药物}主要用于治疗哪些症状？ {药物}主要用于治疗{临床表现}等症状。 {药物}的主要治疗作用是什么？ {药物}的主要治疗作用为治疗{临床表现}。 实验设计 本文涉及了以下四个方面的实验：\n医疗对话实验：作者采用了BLEU、ROUGE和GLEU三个评价指标来评估模型生成的对话质量。实验结果表明，ClinicalGPT在所有ROUGE指标上表现优秀，在BLEU-1和大部分ROUGE指标上也表现出色。 医学考试实验：作者选择了MEDQA-MCMLE数据集中的几个类别进行评估，包括医学伦理、呼吸系统、消化系统等。实验结果显示，ClinicalGPT在各个类别中均表现出色，超过了其他LLMs的平均准确率。 诊断实验：作者使用MD-EHR测试集对模型的诊断能力进行了评估。实验结果表明，ClinicalGPT在各个疾病组别中都表现出色，尤其是在消化系统和泌尿系统方面表现最佳。 医学问答实验：作者使用cMedQA2数据集对模型的问答能力进行了评估。实验结果表明，ClinicalGPT在与BLOOM-7B、LLAMA-7B和ChatGLM-6B的比较中表现最好，尤其在与BLOOM-7B和LLAMA-7B的比较中胜出较多。 总的来说，本文的实验结果表明，ClinicalGPT在医疗领域具有出色的应用潜力，特别是在理解和生成医疗对话、医学检查和诊断等方面。然而，在某些特定领域，如妇科和血液系统，可能需要进一步改进。","title":"CLINICALGPT: LARGE LANGUAGE MODELS FINETUNED WITH DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION"},{"content":"怎么下载更快 因为内地特殊原因，从 hf 上下载模式或者数据集速度不堪入目。很多佬都发过提速方法，这里稍微总结一下。\n来源：\n苏洋博客\n如何快速下载huggingface大模型\nhuggingface-cli命令下载。\n1 huggingface-cli download microsoft/phi-2 但是非常不建议使用这个默认命令下载，会很慢，而且会下在根目录的缓存文件中，并且使用 git blob 来保存模型文件，会比原本模型更大。\n可以在这个命令基础上，使用和指定一些参数来完成高效下载。 比如：\n1 huggingface-cli download microsoft/phi-2 --local-dir=./models/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download --local-dir 和下载使用的缓存目录 --cache-dir都设置到了当前目录下，开启 --local-dir-use-symlinks=False 可以让下载的文件后续都以非软链文件保存，方便后续保存或者上传服务器。最后，添加--resume-download 可以确保始终接着之前的下载进度继续，如果下载进度有中断。\nHF_TRANSFER使用\n是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，开启后在带宽充足的机器上可以跑到 500MB/s。 但是有非常明显的缺点：\nHF_TRANSFER不可断点重续!!!具体说明文档 太容易断了！ 启用方法：export HF_HUB_ENABLE_HF_TRANSFER=1\n镜像网站使用\nhttps://hf-mirror.com，设置\n1 export HF_ENDPOINT=\u0026#34;https://hf-mirror.com\u0026#34; 使用摩搭社区下载\n缺点：模型更新有延迟。\n折腾 hf 下载 使用huggingface-cli+HF_TRANSFER下载。\n具体参数：\n--repo-type：下载类型， \u0026lt;model or dataset name\u0026gt;：模型或者数据集名称， --local-dir：本地位置， --cache-dir：缓存位置， --local-dir-use-symlinks：是否使用软链接。 --resume-download：断点续传。 环境设置：\nexport HF_ENDPOINT=\u0026quot;https://hf-mirror.com\u0026quot; export HF_HUB_ENABLE_HF_TRANSFER=1 bash 脚本自动配置环境和下载 使用方式示例：\n1 bash hf.sh model baichuan-inc/Baichuan2-7B-Chat ./models/baichuan2-7b-chat ./models/cache 1 下载命令参数组装\n需要传入：--repo-type、\u0026lt;model or dataset name\u0026gt;、--local-dir、--cache-dir、--local-dir-use-symlinks参数标志。\n激活 conda 环境\n（个例需要）进入服务器后，因为环境激活问题，直接使用 huggingface-cli命令下载提示找不到 hf-trans，经过排查，需要重新激活 conda 环境。在issue中提到了一种解决方法，在 conda 安装路径下找到/miniconda3/etc/profile.d/conda.sh，使用source命令运行文件。\n由于打开的 bash 可能已经激活了某个环境，所以先deactivate，再activate。\n配置环境\n在服务器是公用的环境下，直接修改环境变量不方便（就🤡），所以需要改动一下环境的时候，就export一下吧。\n判断是否需要hf_transfer\n用transfer是真快，但是吧，服务器网络总开玩笑，下着下着就莫名其妙断了，这种方式还不能断点重连，是真痛苦。我的服务器实践经验，所下文件如果大于 10g，就不用transfer了，慢点就慢点了，目前 2m 左右的速度也能接受。\n指定本地存储、缓存文件\n公用服务器上，希望文件存储到自己文件夹，就指定这两个文件位置，同时把软链接给取消掉，方便移动模型文件位置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash source /home/xx/miniconda3/etc/profile.d/conda.sh conda deactivate conda activate base export HF_ENDPOINT=https://hf-mirror.com if [ \u0026#34;$#\u0026#34; -ne 5 ]; then echo \u0026#34;Usage: $0 \u0026lt;type\u0026gt; \u0026lt;name\u0026gt; \u0026lt;local_dir\u0026gt; \u0026lt;cache_dir\u0026gt; \u0026lt;if_hftrans\u0026gt;\u0026#34; exit 1 fi type=$1 name=$2 local_dir=$3 cache_dir=$4 if_hftrans=$5 if [ \u0026#34;$if_hftrans\u0026#34; -eq 1 ]; then export HF_HUB_ENABLE_HF_TRANSFER=1 export huggingface-cli download \\ --repo-type \u0026#34;${type}\u0026#34; \\ \u0026#34;${name}\u0026#34; \\ --local-dir \u0026#34;${local_dir}\u0026#34; \\ --cache-dir \u0026#34;${cache_dir}\u0026#34; \\ --local-dir-use-symlinks False else export huggingface-cli download \\ --repo-type \u0026#34;${type}\u0026#34; \\ \u0026#34;${name}\u0026#34; \\ --local-dir \u0026#34;${local_dir}\u0026#34; \\ --cache-dir \u0026#34;${cache_dir}\u0026#34; \\ --local-dir-use-symlinks False \\ --resume-download fi references Can’t execute conda activate from bash script · Issue #7980 · conda/conda 节省时间：AI 模型靠谱下载方案汇总 如何快速下载huggingface大模型 – padeoe的小站 HF-Mirror https://huggingface.co/docs/huggingface_hub/v0.23.4/package_reference/environment_variables#hfhubenablehftransfer ","permalink":"https://tanxiangyuu.github.io/posts/tech/hf_download/","summary":"怎么下载更快 因为内地特殊原因，从 hf 上下载模式或者数据集速度不堪入目。很多佬都发过提速方法，这里稍微总结一下。\n来源：\n苏洋博客\n如何快速下载huggingface大模型\nhuggingface-cli命令下载。\n1 huggingface-cli download microsoft/phi-2 但是非常不建议使用这个默认命令下载，会很慢，而且会下在根目录的缓存文件中，并且使用 git blob 来保存模型文件，会比原本模型更大。\n可以在这个命令基础上，使用和指定一些参数来完成高效下载。 比如：\n1 huggingface-cli download microsoft/phi-2 --local-dir=./models/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download --local-dir 和下载使用的缓存目录 --cache-dir都设置到了当前目录下，开启 --local-dir-use-symlinks=False 可以让下载的文件后续都以非软链文件保存，方便后续保存或者上传服务器。最后，添加--resume-download 可以确保始终接着之前的下载进度继续，如果下载进度有中断。\nHF_TRANSFER使用\n是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，开启后在带宽充足的机器上可以跑到 500MB/s。 但是有非常明显的缺点：\nHF_TRANSFER不可断点重续!!!具体说明文档 太容易断了！ 启用方法：export HF_HUB_ENABLE_HF_TRANSFER=1\n镜像网站使用\nhttps://hf-mirror.com，设置\n1 export HF_ENDPOINT=\u0026#34;https://hf-mirror.com\u0026#34; 使用摩搭社区下载\n缺点：模型更新有延迟。\n折腾 hf 下载 使用huggingface-cli+HF_TRANSFER下载。\n具体参数：\n--repo-type：下载类型， \u0026lt;model or dataset name\u0026gt;：模型或者数据集名称， --local-dir：本地位置， --cache-dir：缓存位置， --local-dir-use-symlinks：是否使用软链接。 --resume-download：断点续传。 环境设置：\nexport HF_ENDPOINT=\u0026quot;https://hf-mirror.com\u0026quot; export HF_HUB_ENABLE_HF_TRANSFER=1 bash 脚本自动配置环境和下载 使用方式示例：","title":"从 huggingface 下载文件到公共服务器"},{"content":"内网穿透介绍 互联网上两个不同的主机进行通信首先需要知道对方IP。根据IP协议，只有分配了公网IP的设备才能在互联网上通信和传输数据。而中国人口/设备众多，分配到的IPv4资源又少，因此绝大部分情况是通过路由器/交换机转换公网IP后才上网。 位于路由器/交换机后的设备一般是内网设备，分配的IP地址以 192.168/172.16/10.0 开头，属于内网IP。要让内网设备对外提供服务，就需要进行内网穿透。\nfrp介绍 frp 是一个开源、简洁易用、高性能的内网穿透和反向代理软件，支持 tcp, udp, http, https等协议。 frp 项目官网是 GitHub - fatedier/frp，中文官方文档地址：frp/README_zh.md。除了安装过程，中文文档对使用过程已经介绍的非常详细，如遇到问题，建议先查看官方文档。 frp工作原理为：\n服务端运行，监听一个主端口，等待客户端的连接； 客户端连接到服务端的主端口，同时告诉服务端要监听的端口和转发类型； 服务端fork新的进程监听客户端指定的端口； 外网用户连接到客户端指定的端口，服务端通过和客户端的连接将数据转发到客户端； 客户端进程再将数据转发到本地服务，从而实现内网对外暴露服务的能力。 frp内网穿透教程 部署frp服务端（外网机器） 在frp下载界面，下载 frp，注意根据机器类型选择版本，这里因为我的客户端有人已经装了 frp_0.33.0_linux_amd64.tar.gz，所以在服务端也下载这个版本。 解压安装包：tar -zxvf frp_0.33.0_linux_amd64.tar.gz 进入解压目录：cd frp_0.33.0_linux_amd64，编辑 frps.ini文件，编辑完之后删除注释！！！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [common] # frp监听的端口，默认是7000，可以改成其他的 bind_port = 7000 # 授权码，请改成更复杂的 token = 12345678 # frp管理后台端口，请按自己需求更改 dashboard_port = 7500 # frp管理后台用户名和密码，请改成自己的 dashboard_user = admin dashboard_pwd = admin enable_prometheus = true # frp日志配置 log_file = /var/log/frps.log log_level = info log_max_days = 3 设置 frp 服务和开机自启动服务。注意以下 bash 代码需要在进入 frp 文件目录之后完成。 1 2 3 4 5 6 mkdir -p /etc/frp cp frps.ini /etc/frp cp frps /usr/bin cp systemd/frps.service /usr/lib/systemd/system/ systemctl enable frps systemctl start frps 确保监听端口：如 7000，和管理后台端口：如 7500，被服务器放开。 如没有放开，在 centos 系统下，防火墙放行端口： 1 2 3 4 5 # 添加监听端口 firewall-cmd --permanent --add-port=7000/tcp # 添加管理后台端口 firewall-cmd --permanent --add-port=7500/tcp firewall-cmd --reload ubuntu：ufw 设置防火墙。\n浏览器打开“http://服务器IP:后台管理端口” ，输入用户名和密码可以查看连接状态： 配置 frp 客户端（内网服务器），并且适配多个服务端 下载frp下载页面，我这边内网服务器上已经有人下载了 33 版本。 解压缩，进入文件夹内 编辑frpc.ini文件，按照需求转发，因为内网服务器上有人装了 frp 了，所以 frpc.ini文件他已经编辑过了，而一个frpc.ini文件只能设置一个服务端，和多个客户端隧道，这时候不能动人家的文件。只能自己新添加一个frpc1.ini文件，设置好之后添加新的服务。 frpc1.ini文件配置： 1 2 3 4 5 6 7 8 9 10 11 12 [common] server_addr = 服务器ip #设置的端口 server_port = 7000 token = 12345678 #配置客户端ssh服务 [ssh1] # ssh1 服务名，每个服务名字必须唯一，也不能和其他frpc.ini文件里的服务名冲突 type = tcp local_ip = 127.0.0.1 local_port = 22 #内网服务器ssh服务端口，可以设置为其他端口，但请保证其他端口开通了ssh连接权限 remote_port = 8080 #自定义的远程服务器端口，也就是以后通过这个端口进行ssh连接，好像一个伪装一样。 防火墙放行端口 设置 frp 服务，和开机自启动服务。 正常情况下，应该新添加一个服务 **frpc1.service**，为每个 frp 示例创建一个新的独立服务单元，但是我的服务器上，并没有激活 frpc.service 服务，原本用户应该是用nohup直接挂载了 frpc.ini 文件。所以我直接修改了 frpc.service 文件。 需要将frpc.service中的 service 下面重新启动 frpc.ini 文件改为 frpc1.ini文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=Frp Client Service After=network.target [Service] Type=simple User=nobody Restart=on-failure RestartSec=5s ExecStart=/usr/bin/frpc -c /etc/frp/frpc.ini # 改为frpc1.ini ExecReload=/usr/bin/frpc reload -c /etc/frp/frpc.ini # 改为frpc1.ini [Install] WantedBy=multi-user.target 1 2 3 4 5 6 mkdir -p /etc/frp cp frpc1.ini /etc/frp cp frpc /usr/bin cp systemd/frpc.service /usr/lib/systemd/system/ systemctl enable frpc systemctl start frpc 以上命令代码需要在sudo模式下执行。\n登录frp管理后台，观察客户端是否已经连上来。 使用 ssh 命令连接客户端 ssh usr@x.x.x.x -p yyyy\nusr：客户端用户名 x.x.x.x: 服务端 ip yyyy：remote_port 所指端口。 ","permalink":"https://tanxiangyuu.github.io/posts/tech/frp/","summary":"内网穿透介绍 互联网上两个不同的主机进行通信首先需要知道对方IP。根据IP协议，只有分配了公网IP的设备才能在互联网上通信和传输数据。而中国人口/设备众多，分配到的IPv4资源又少，因此绝大部分情况是通过路由器/交换机转换公网IP后才上网。 位于路由器/交换机后的设备一般是内网设备，分配的IP地址以 192.168/172.16/10.0 开头，属于内网IP。要让内网设备对外提供服务，就需要进行内网穿透。\nfrp介绍 frp 是一个开源、简洁易用、高性能的内网穿透和反向代理软件，支持 tcp, udp, http, https等协议。 frp 项目官网是 GitHub - fatedier/frp，中文官方文档地址：frp/README_zh.md。除了安装过程，中文文档对使用过程已经介绍的非常详细，如遇到问题，建议先查看官方文档。 frp工作原理为：\n服务端运行，监听一个主端口，等待客户端的连接； 客户端连接到服务端的主端口，同时告诉服务端要监听的端口和转发类型； 服务端fork新的进程监听客户端指定的端口； 外网用户连接到客户端指定的端口，服务端通过和客户端的连接将数据转发到客户端； 客户端进程再将数据转发到本地服务，从而实现内网对外暴露服务的能力。 frp内网穿透教程 部署frp服务端（外网机器） 在frp下载界面，下载 frp，注意根据机器类型选择版本，这里因为我的客户端有人已经装了 frp_0.33.0_linux_amd64.tar.gz，所以在服务端也下载这个版本。 解压安装包：tar -zxvf frp_0.33.0_linux_amd64.tar.gz 进入解压目录：cd frp_0.33.0_linux_amd64，编辑 frps.ini文件，编辑完之后删除注释！！！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [common] # frp监听的端口，默认是7000，可以改成其他的 bind_port = 7000 # 授权码，请改成更复杂的 token = 12345678 # frp管理后台端口，请按自己需求更改 dashboard_port = 7500 # frp管理后台用户名和密码，请改成自己的 dashboard_user = admin dashboard_pwd = admin enable_prometheus = true # frp日志配置 log_file = /var/log/frps.","title":"Frp"},{"content":"对于堆来说，有几个重要的点，虽然简单，但是需要理解，才能理解堆\n完全二叉树 堆是一个完全二叉树 可以分成，大顶堆和小顶堆 大顶堆：父节点大于等于子节点 小顶堆：父节点小于等于子节点 堆的存储：数组形式\u0026ndash;利用完全二叉树的性质。 ","permalink":"https://tanxiangyuu.github.io/posts/algo/%E5%A0%86%E6%8E%92%E5%BA%8F/","summary":"对于堆来说，有几个重要的点，虽然简单，但是需要理解，才能理解堆\n完全二叉树 堆是一个完全二叉树 可以分成，大顶堆和小顶堆 大顶堆：父节点大于等于子节点 小顶堆：父节点小于等于子节点 堆的存储：数组形式\u0026ndash;利用完全二叉树的性质。 ","title":"堆排序"},{"content":"题目：假设有一组无序的数字，找到其中排名第 k 位的数字。\n快排 快速排序是一种优秀的排序算法。\nC++ STL 的 sort，使用的就是“快速排序 + 插入排序 + 堆排序”的方式。\n快排的核心 找准基准值的位置\n通过的是partition操作，将数组分为两部分，小于基准值的放在左边，大于基准值的放在右边。\n然后通过递归，对两边继续进行partition操作。\n问题\n求排名为k的元素，并不需要对整个数组进行排序。 时间复杂度不稳定，最坏的情况会达到O(n^2)。 快速选择排序 针对第一个问题：使用快速选择排序方法解决。在不对数据整体进行排序的前提下，快速找到排名第 k 位的元素，而且时间复杂度还能优化到 O(n)。\n快速选择的核心 当需要快速找到一个元素 X，并且使得小于 X 的元素数量是 k-1 个时，那 X 就是要查找的排名第 k 位的元素了。\n没有必要对整个数组进行排序。\n依旧使用partition操作进行实现。 在partition操作中，将基准值排名ind和 k 进行比较。\n如果 ind 正好等于 k，那说明当前的基准值，就是要找的排名第 k 位的元素； 如果 ind 大于 k，说明排名第 k 位的元素在基准值的前面。接下来，要解决的问题就是，在基准值的前面查找排名第 k 位的元素； 如果 ind 小于 k ，就说明排名第 k 位的元素在基准值的后面，并且，当前包括基准值在内的 ind 个元素，都是小于基准值的元素。那么，问题就转化成了，在基准值的后面查找排名第 k - ind 位的元素。 三种快排的优化 针对第二个问题，稳定系统运行时间。\n三种方法 优化1：单边递归优化 在快排的实现中，使用的是双边递归。如代码：\n1 2 quick_sort(arr, l, x - 1); // 对左半边排序 quick_sort(arr, x + 1 , r); // 对右半边排序 从程序的运行时间来考虑的话，每次函数调用，都会消耗掉一部分运行时间。那只要可以减少函数调用的次数，其实就可以加快一点程序运行的速度。\n单边递归：代码：\n1 2 3 4 5 6 7 8 9 10 11 12 void quick_sort(int *arr, int l, int r) { while (l \u0026lt; r) { // 进行一轮 partition 操作 // 获得基准值的位置 int ind = partition(arr, l, r); // 右侧正常调用递归函数 quick_sort(arr, ind + 1, r); // 用本层处理左侧的排序 r = ind - 1; } return ; } 优化2：三数取中优化-基准值选取优化 基准值选取不合理，会导致算法效率的降低。只有当基准值每次都能将排序区间中的数据平分时，时间复杂度才是最好情况下的 O(nlogn)。\n所谓三点取中法，就是每一轮取排序区间的头、尾和中间元素这三个值，然后把它们排序以后的中间值作为本轮的基准值。\n优化3：partition 操作优化 核心思想：头指针找小值，尾指针找大值，然后交换。免去填空过程。\n补充 在运用快速选择算法以寻找排名第 k 位置的元素时，实际上，一旦我们借助该算法确定了第 k 位元素的值，将这个值与它之前的元素值累加起来，即可得到前 k 位元素的所有值。换言之，快速选择算法不仅能有效解决寻找单个第 k 位元素的问题，还能进一步应用于解决求解前 k 个小或前 k 个大元素等更为广泛的 Top-K 问题。\n","permalink":"https://tanxiangyuu.github.io/posts/algo/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/","summary":"题目：假设有一组无序的数字，找到其中排名第 k 位的数字。\n快排 快速排序是一种优秀的排序算法。\nC++ STL 的 sort，使用的就是“快速排序 + 插入排序 + 堆排序”的方式。\n快排的核心 找准基准值的位置\n通过的是partition操作，将数组分为两部分，小于基准值的放在左边，大于基准值的放在右边。\n然后通过递归，对两边继续进行partition操作。\n问题\n求排名为k的元素，并不需要对整个数组进行排序。 时间复杂度不稳定，最坏的情况会达到O(n^2)。 快速选择排序 针对第一个问题：使用快速选择排序方法解决。在不对数据整体进行排序的前提下，快速找到排名第 k 位的元素，而且时间复杂度还能优化到 O(n)。\n快速选择的核心 当需要快速找到一个元素 X，并且使得小于 X 的元素数量是 k-1 个时，那 X 就是要查找的排名第 k 位的元素了。\n没有必要对整个数组进行排序。\n依旧使用partition操作进行实现。 在partition操作中，将基准值排名ind和 k 进行比较。\n如果 ind 正好等于 k，那说明当前的基准值，就是要找的排名第 k 位的元素； 如果 ind 大于 k，说明排名第 k 位的元素在基准值的前面。接下来，要解决的问题就是，在基准值的前面查找排名第 k 位的元素； 如果 ind 小于 k ，就说明排名第 k 位的元素在基准值的后面，并且，当前包括基准值在内的 ind 个元素，都是小于基准值的元素。那么，问题就转化成了，在基准值的后面查找排名第 k - ind 位的元素。 三种快排的优化 针对第二个问题，稳定系统运行时间。","title":"快速选择排序"},{"content":"RAG-Retrieval Augmented Generation 1. RAG介绍 RAG，检索增强生成技术，是一种基于检索的生成模型，它结合了生成模型的灵活性和检索模型的效率。RAG通过将生成模型与检索模型相结合，实现了高效的文本生成。\n目的：解决LLM的幻觉情况，加深专业领域深度，更新知识库。\n最小RAG的基本结构：\n向量化模块，将文档（文本）片段向量化。 文档加载和切分模块，加载文档和切分文档片段。 数据库模块，存放切分好的文档片段及其对应的向量表示 检索模块，根据query检索相应的文档片段。 大模型模块，根据检索出的文档来回答query 生成answer。 RAG的流程 ：\n索引：将文档库分割成较短的chunk，并且通过编码器构建向量索引。 检索：根据query和chunks的相似度检索相关文档片段。 生成：通过检索到的文档，作为上下文条件，生成answer。 2. 各个模块的code demo code 待补充。。。\n3. 总结 一个最小的RAG包括：\n向量化模块 文档加载和切分模块 数据库 检索模块–向量 大模型模块 4. 论文 [1] RAG: Retrieval Augmented Generation for Dense Text-to-Text Pre-training. ","permalink":"https://tanxiangyuu.github.io/posts/llm/rag/","summary":"RAG-Retrieval Augmented Generation 1. RAG介绍 RAG，检索增强生成技术，是一种基于检索的生成模型，它结合了生成模型的灵活性和检索模型的效率。RAG通过将生成模型与检索模型相结合，实现了高效的文本生成。\n目的：解决LLM的幻觉情况，加深专业领域深度，更新知识库。\n最小RAG的基本结构：\n向量化模块，将文档（文本）片段向量化。 文档加载和切分模块，加载文档和切分文档片段。 数据库模块，存放切分好的文档片段及其对应的向量表示 检索模块，根据query检索相应的文档片段。 大模型模块，根据检索出的文档来回答query 生成answer。 RAG的流程 ：\n索引：将文档库分割成较短的chunk，并且通过编码器构建向量索引。 检索：根据query和chunks的相似度检索相关文档片段。 生成：通过检索到的文档，作为上下文条件，生成answer。 2. 各个模块的code demo code 待补充。。。\n3. 总结 一个最小的RAG包括：\n向量化模块 文档加载和切分模块 数据库 检索模块–向量 大模型模块 4. 论文 [1] RAG: Retrieval Augmented Generation for Dense Text-to-Text Pre-training. ","title":"RAG"},{"content":"本文是针对PyTorch中的张量（Tensors）的一个基础教程，它详细介绍了张量的定义、特性、以及如何在PyTorch中使用张量进行基本操作。张量是PyTorch中进行科学计算的基础，它们可以视为一个高维数组或矩阵。本教程的主要内容包括：\n张量初始化 从数据直接创建张量：可以直接从数据创建张量，PyTorch会自动推断数据类型。 从NumPy数组创建：可以使用torch.from_numpy()从NumPy数组创建张量。 通过已有的张量创建：可以通过已有的张量来创建新的张量。这种方法会默认重用输入张量的属性（如数据类型），除非显式地进行更改。 使用随机或常数值：torch.rand()创建随机初始化的张量，torch.zeros()和torch.ones()分别创建全0或全1的张量。 张量属性 张量属性：张量的属性包括形状（shape）、数据类型（dtype）和存储的设备（device，如CPU或GPU）。 张量操作 索引和切片：可以使用标准的Python索引和切片操作来访问张量的部分。 张量重塑：reshape可以改变张量的形状而不改变其数据。 张量合并：torch.cat可以用来在给定维度上合并张量序列。 张量乘法：介绍了元素乘法（*或torch.mul）和矩阵乘法（@或torch.matmul）。 张量与NumPy之间的转换 张量可以很容易地与NumPy数组相互转换，使用numpy()方法从张量转换为NumPy数组，使用torch.from_numpy()从NumPy数组转换为张量。这两种类型的转换是共享底层内存的，因此修改其中一个会影响另一个。 自动微分 自动微分：PyTorch中的自动微分是通过autograd模块实现的，它提供了张量上所有操作的自动微分。这对于深度学习训练中的梯度计算非常有用。 运行在GPU上 张量可以被移到GPU上：使用.to方法可以将张量移动到任何设备上，这对于加速计算非常重要。 本教程适合初学者，通过详细的示例和解释，帮助读者理解和掌握如何在PyTorch中有效地使用张量进行数据操作和计算。\n","permalink":"https://tanxiangyuu.github.io/posts/llm/pytorch/","summary":"本文是针对PyTorch中的张量（Tensors）的一个基础教程，它详细介绍了张量的定义、特性、以及如何在PyTorch中使用张量进行基本操作。张量是PyTorch中进行科学计算的基础，它们可以视为一个高维数组或矩阵。本教程的主要内容包括：\n张量初始化 从数据直接创建张量：可以直接从数据创建张量，PyTorch会自动推断数据类型。 从NumPy数组创建：可以使用torch.from_numpy()从NumPy数组创建张量。 通过已有的张量创建：可以通过已有的张量来创建新的张量。这种方法会默认重用输入张量的属性（如数据类型），除非显式地进行更改。 使用随机或常数值：torch.rand()创建随机初始化的张量，torch.zeros()和torch.ones()分别创建全0或全1的张量。 张量属性 张量属性：张量的属性包括形状（shape）、数据类型（dtype）和存储的设备（device，如CPU或GPU）。 张量操作 索引和切片：可以使用标准的Python索引和切片操作来访问张量的部分。 张量重塑：reshape可以改变张量的形状而不改变其数据。 张量合并：torch.cat可以用来在给定维度上合并张量序列。 张量乘法：介绍了元素乘法（*或torch.mul）和矩阵乘法（@或torch.matmul）。 张量与NumPy之间的转换 张量可以很容易地与NumPy数组相互转换，使用numpy()方法从张量转换为NumPy数组，使用torch.from_numpy()从NumPy数组转换为张量。这两种类型的转换是共享底层内存的，因此修改其中一个会影响另一个。 自动微分 自动微分：PyTorch中的自动微分是通过autograd模块实现的，它提供了张量上所有操作的自动微分。这对于深度学习训练中的梯度计算非常有用。 运行在GPU上 张量可以被移到GPU上：使用.to方法可以将张量移动到任何设备上，这对于加速计算非常重要。 本教程适合初学者，通过详细的示例和解释，帮助读者理解和掌握如何在PyTorch中有效地使用张量进行数据操作和计算。","title":"PyTorch张量基础教程总结"},{"content":"原文链接\n引言 尊重与期待：黑客欣赏高质量问题，对好问题持积极态度，但对不假思索或懒惰提问者有所抵触。 提问前的准备工作 自行寻找答案 搜索论坛帖子 使用搜索引擎查询 阅读手册与常见问题解答(FAQ) 自行尝试和测试 向身边专家咨询 如是程序员，阅读源代码 展示前期努力：在提问时，告知你已经尝试过的解决途径，证明你不是一个坐享其成者。 提问技巧 选择合适的提问场所：避免在不相关的论坛提问，查找并选用专门针对你问题的主题论坛或邮件列表。 利用Stack Overflow和Stack Exchange：在提问前先搜索，然后在对应主题的Stack Exchange网站上提问。 邮件列表与IRC：参加项目邮件列表，若有必要，可在开发者邮件列表提问，前提是你已经尝试过用户列表。 编写有效的标题： 标题示例：目标——差异式描述 示例：X.org 6.8.1鼠标指针在MV1005显卡芯片组下的变形问题 方便回复：不要请求直接回复到个人邮箱，而是使用恰当的论坛或邮件列表提问，并确保标题能准确反映问题内容。 清晰表述： 使用正确、清晰、准确的语言，避免拼写、语法错误。 尊重非母语使用者，但力求表达清晰，必要时声明语言障碍。 便于阅读的格式： 使用纯文本格式，避免HTML和特殊格式。 不要发送封闭格式的文档，如Word或Excel文件。 在论坛中适度使用表情符号和格式，保持专业简洁。 问题描述详尽： 描述问题的具体表现。 提供环境信息，如操作系统、应用版本等。 描述已经进行的研究与诊断步骤。 提供可能导致问题变化的背景信息。 提供重现问题的简明步骤。 精炼问题：提供简明扼要且具有针对性的信息，尽量缩小问题范围。 谨慎声称发现Bug： 在确定是Bug之前，先假设可能自身操作不当。 提供详尽的重现步骤及可能的补丁或回归测试证据。 低声下气无助于解决问题 避免以过分谦卑或模糊的方式提出问题，应清晰表述背景及问题细节，不必自我贬低。明确问题的具体症状而非主观猜测，以便他人准确判断和诊断。 提供精确的技术信息 当提问关于技术问题时，务必提供详尽的硬件配置、软件环境及错误现象等信息，如编译错误的例子所示，确保对方能看到与你相同的现象，而非仅听信个人揣测。 遵循问题发生的时间顺序 记录问题出现前的操作流程，按时间顺序详细描述，包括系统的反应和出现问题的具体环节。如有必要，提供调试日志和相关设置，确保信息具有针对性和实用性。 聚焦问题实质而非操作过程 提问时首先阐明你的最终目标，而非纠结于某一无效步骤。例如，当询问颜色选择器获取RGB值时，应说明实际需要达成的任务，以便得到更有效的解决方案。 鼓励公开交流 避免请求私下回复，提倡在公共平台讨论问题，以便更多人参与、纠正错误，并为解答者带来社区认可。只有在特殊情况下，如预期收到大量重复回复时，可提议私下整合信息后回馈至公共平台。 清晰简洁地表述需求 明确告知需要何种形式的帮助，限制所需投入的时间和精力，使专家更容易给出有针对性的回答。简化问题表述，比如求教参考资料来源优于直接要求解释概念。 提供最小化复现代码 当涉及编程问题时，附上精简且能够复现问题的代码片段，明确指出期望结果和实际结果之间的差异。创建最精简的测试用例有助于他人快速定位问题，从而提高获得有效回复的可能性。 对待家庭作业的态度 对于疑似家庭作业性质的问题，黑客社群鼓励自行解决以积累经验。可以请求提示，但不应索要完整答案。在适当场合如用户论坛中提问时，表明你已尽力但仍需指导，或许能得到有益的提示。 去除无意义提问以提升问题质量 避免使用无助于解决问题的结尾疑问句，如\u0026quot;有人能帮我吗？\u0026quot; 不明确的问题加上这类问句反而显多余，易遭黑客社区反感 尽量减少\u0026quot;是/否\u0026quot;型提问，除非期望得到确切的二元答复 慎用“紧急”标签以获取有效关注 标注“紧急”往往适得其反，可能导致问题被忽视或删除 在非专业场合高调标榜紧急可能因垃圾信息过滤而失效 特殊情况下的紧急提及须礼貌且基于共同兴趣点 礼仪助力沟通，提升获得解答的可能性 表达感谢，如使用“请”和“谢谢您的关注”，展示对他人力气的认可 礼貌虽非首要，却有益于问题得到关注，尤其在连续提问情境下 先致谢后再次感谢回复者的具体帮助，避免误解 问题解决后的跟进与分享 解决问题后及时告知并感谢参与者 使用“已解决”等标记更新原话题标题，方便他人查阅 简洁概述问题解决方法及原因，提及关键协助者 提供简洁的调试摘要或指出避免问题的途径 正确反馈问题解决状态有助于满足解答者成就感 考虑编写文档或添加FAQ，防止他人重复遇到相同问题 如何解读答案 RTFM 和 STFW：遭遇经典回应的意义与应对 当收到RTFM或STFW回复时，这意味着你应该查阅相关手册或自行上网搜索。 这类回答表明信息易于获取，自行搜索能促进学习，而非直接给予答案。 收到此类回复应视为对方已在某种程度上提供了关注，对此表示感谢。 面对困惑时的正确求解姿态 若无法理解回复，尝试自行解决，参考手册、FAQ、网络资源或请教他人。 提出疑问时展现自主探索过程，如举例询问具体的细节而非基础概念。 应对无礼回应与黑客文化 黑客圈内的直率交流可能被误认为无礼，遇此情况保持冷静。 如确实遭受冒犯，社群内其他成员可能会介入纠正不当行为。 对于真正的冒犯者，有力反击可被接受，但新手需谨慎判断，避免陷入无谓争执。 黑客文化中的某些特质可能源于独特的社交习惯，不必过分解读。 Jeff Bigler 的观点：应对策略 Jeff Bigler 关于“社交过滤器”的观察可作为理解和适应黑客社群互动方式的参考。 如何避免扮演失败者 面对批评与纠正 接受并忍受在黑客社区中因失误而受到的公开批评，这是社区标准得以维持的方式。不应期待所有意见都通过私人途径传达，避免将建设性批评视为个人攻击。 处理挑衅行为 遇到无端指责时，保持冷静，避免陷入口水战。黑客指出错误是出于关心社区和个人成长，不应为此抱怨或要求特殊对待。 学会区分口水战与实质性回复 多数口水战无需理会，确认其中是否包含对问题实质的解答或有价值的建议。 不该问的问题及其典型回应 寻找资源路径 利用搜索引擎寻找所需程序或资源，基本查询能力应当具备。 操作方法问题 提问过于宽泛，未能明确问题焦点，建议先自行研究Y问题的本质。 配置问题 自行阅读手册（RTFM），自行查找答案。 文件转换问题 自行尝试并验证可行性。 无效的程序/设置问题表述 提供具体问题详情，避免空泛陈述。 针对Windows的问题 优先考虑更换操作系统或在适当场合提问，如涉及与开源软件交互问题。 质疑系统工具有效性 确保问题归因准确，提供详细证据。 安装Linux或其他操作系统问题 寻求本地用户群组的帮助，提供具体故障细节。 非法活动请求 黑客不会支持非法行为，此类问题不会得到回答。 好问题与蠢问题的区别 展示搜索努力 聪明的问题表明提问者已经尽力搜索但仍未解决问题，寻求进一步指引。 尊重他人时间 避免责备他人或表现出傲慢态度，详细描述问题背景和已尝试的解决方案。 得不到回答时的对策 保持耐心 问题可能因多种原因暂时未得到回答，重复张贴问题非明智之举。 寻求其他援助渠道 加入用户群组或寻求商业支持，理解免费技术支持的局限性。 如何更好地回答问题 友好态度 保持礼貌和善，理解提问者可能承受的压力。 私下回复初犯者 对真诚的新手私下指导，避免公开羞辱。 不确定时明确表态 避免给出错误答案，鼓励提问者提供更多细节。 避免误导性玩笑 不要给出可能破坏提问者设置的玩笑性建议。 引导提问者细化问题 通过反问引导提问者提供更多细节，促进双方学习。 给出高质量答案 避免给出权宜之计，推荐更好的工具或重新定义问题。 正面回答并分享技巧 在回答时强调解决问题的方法，而非单纯提供结果。 推动社区进步 从问题中总结经验教训，改进文档和常见问题解答，以便未来参考。 ","permalink":"https://tanxiangyuu.github.io/posts/tech/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/","summary":"原文链接\n引言 尊重与期待：黑客欣赏高质量问题，对好问题持积极态度，但对不假思索或懒惰提问者有所抵触。 提问前的准备工作 自行寻找答案 搜索论坛帖子 使用搜索引擎查询 阅读手册与常见问题解答(FAQ) 自行尝试和测试 向身边专家咨询 如是程序员，阅读源代码 展示前期努力：在提问时，告知你已经尝试过的解决途径，证明你不是一个坐享其成者。 提问技巧 选择合适的提问场所：避免在不相关的论坛提问，查找并选用专门针对你问题的主题论坛或邮件列表。 利用Stack Overflow和Stack Exchange：在提问前先搜索，然后在对应主题的Stack Exchange网站上提问。 邮件列表与IRC：参加项目邮件列表，若有必要，可在开发者邮件列表提问，前提是你已经尝试过用户列表。 编写有效的标题： 标题示例：目标——差异式描述 示例：X.org 6.8.1鼠标指针在MV1005显卡芯片组下的变形问题 方便回复：不要请求直接回复到个人邮箱，而是使用恰当的论坛或邮件列表提问，并确保标题能准确反映问题内容。 清晰表述： 使用正确、清晰、准确的语言，避免拼写、语法错误。 尊重非母语使用者，但力求表达清晰，必要时声明语言障碍。 便于阅读的格式： 使用纯文本格式，避免HTML和特殊格式。 不要发送封闭格式的文档，如Word或Excel文件。 在论坛中适度使用表情符号和格式，保持专业简洁。 问题描述详尽： 描述问题的具体表现。 提供环境信息，如操作系统、应用版本等。 描述已经进行的研究与诊断步骤。 提供可能导致问题变化的背景信息。 提供重现问题的简明步骤。 精炼问题：提供简明扼要且具有针对性的信息，尽量缩小问题范围。 谨慎声称发现Bug： 在确定是Bug之前，先假设可能自身操作不当。 提供详尽的重现步骤及可能的补丁或回归测试证据。 低声下气无助于解决问题 避免以过分谦卑或模糊的方式提出问题，应清晰表述背景及问题细节，不必自我贬低。明确问题的具体症状而非主观猜测，以便他人准确判断和诊断。 提供精确的技术信息 当提问关于技术问题时，务必提供详尽的硬件配置、软件环境及错误现象等信息，如编译错误的例子所示，确保对方能看到与你相同的现象，而非仅听信个人揣测。 遵循问题发生的时间顺序 记录问题出现前的操作流程，按时间顺序详细描述，包括系统的反应和出现问题的具体环节。如有必要，提供调试日志和相关设置，确保信息具有针对性和实用性。 聚焦问题实质而非操作过程 提问时首先阐明你的最终目标，而非纠结于某一无效步骤。例如，当询问颜色选择器获取RGB值时，应说明实际需要达成的任务，以便得到更有效的解决方案。 鼓励公开交流 避免请求私下回复，提倡在公共平台讨论问题，以便更多人参与、纠正错误，并为解答者带来社区认可。只有在特殊情况下，如预期收到大量重复回复时，可提议私下整合信息后回馈至公共平台。 清晰简洁地表述需求 明确告知需要何种形式的帮助，限制所需投入的时间和精力，使专家更容易给出有针对性的回答。简化问题表述，比如求教参考资料来源优于直接要求解释概念。 提供最小化复现代码 当涉及编程问题时，附上精简且能够复现问题的代码片段，明确指出期望结果和实际结果之间的差异。创建最精简的测试用例有助于他人快速定位问题，从而提高获得有效回复的可能性。 对待家庭作业的态度 对于疑似家庭作业性质的问题，黑客社群鼓励自行解决以积累经验。可以请求提示，但不应索要完整答案。在适当场合如用户论坛中提问时，表明你已尽力但仍需指导，或许能得到有益的提示。 去除无意义提问以提升问题质量 避免使用无助于解决问题的结尾疑问句，如\u0026quot;有人能帮我吗？\u0026quot; 不明确的问题加上这类问句反而显多余，易遭黑客社区反感 尽量减少\u0026quot;是/否\u0026quot;型提问，除非期望得到确切的二元答复 慎用“紧急”标签以获取有效关注 标注“紧急”往往适得其反，可能导致问题被忽视或删除 在非专业场合高调标榜紧急可能因垃圾信息过滤而失效 特殊情况下的紧急提及须礼貌且基于共同兴趣点 礼仪助力沟通，提升获得解答的可能性 表达感谢，如使用“请”和“谢谢您的关注”，展示对他人力气的认可 礼貌虽非首要，却有益于问题得到关注，尤其在连续提问情境下 先致谢后再次感谢回复者的具体帮助，避免误解 问题解决后的跟进与分享 解决问题后及时告知并感谢参与者 使用“已解决”等标记更新原话题标题，方便他人查阅 简洁概述问题解决方法及原因，提及关键协助者 提供简洁的调试摘要或指出避免问题的途径 正确反馈问题解决状态有助于满足解答者成就感 考虑编写文档或添加FAQ，防止他人重复遇到相同问题 如何解读答案 RTFM 和 STFW：遭遇经典回应的意义与应对 当收到RTFM或STFW回复时，这意味着你应该查阅相关手册或自行上网搜索。 这类回答表明信息易于获取，自行搜索能促进学习，而非直接给予答案。 收到此类回复应视为对方已在某种程度上提供了关注，对此表示感谢。 面对困惑时的正确求解姿态 若无法理解回复，尝试自行解决，参考手册、FAQ、网络资源或请教他人。 提出疑问时展现自主探索过程，如举例询问具体的细节而非基础概念。 应对无礼回应与黑客文化 黑客圈内的直率交流可能被误认为无礼，遇此情况保持冷静。 如确实遭受冒犯，社群内其他成员可能会介入纠正不当行为。 对于真正的冒犯者，有力反击可被接受，但新手需谨慎判断，避免陷入无谓争执。 黑客文化中的某些特质可能源于独特的社交习惯，不必过分解读。 Jeff Bigler 的观点：应对策略 Jeff Bigler 关于“社交过滤器”的观察可作为理解和适应黑客社群互动方式的参考。 如何避免扮演失败者 面对批评与纠正 接受并忍受在黑客社区中因失误而受到的公开批评，这是社区标准得以维持的方式。不应期待所有意见都通过私人途径传达，避免将建设性批评视为个人攻击。 处理挑衅行为 遇到无端指责时，保持冷静，避免陷入口水战。黑客指出错误是出于关心社区和个人成长，不应为此抱怨或要求特殊对待。 学会区分口水战与实质性回复 多数口水战无需理会，确认其中是否包含对问题实质的解答或有价值的建议。 不该问的问题及其典型回应 寻找资源路径 利用搜索引擎寻找所需程序或资源，基本查询能力应当具备。 操作方法问题 提问过于宽泛，未能明确问题焦点，建议先自行研究Y问题的本质。 配置问题 自行阅读手册（RTFM），自行查找答案。 文件转换问题 自行尝试并验证可行性。 无效的程序/设置问题表述 提供具体问题详情，避免空泛陈述。 针对Windows的问题 优先考虑更换操作系统或在适当场合提问，如涉及与开源软件交互问题。 质疑系统工具有效性 确保问题归因准确，提供详细证据。 安装Linux或其他操作系统问题 寻求本地用户群组的帮助，提供具体故障细节。 非法活动请求 黑客不会支持非法行为，此类问题不会得到回答。 好问题与蠢问题的区别 展示搜索努力 聪明的问题表明提问者已经尽力搜索但仍未解决问题，寻求进一步指引。 尊重他人时间 避免责备他人或表现出傲慢态度，详细描述问题背景和已尝试的解决方案。 得不到回答时的对策 保持耐心 问题可能因多种原因暂时未得到回答，重复张贴问题非明智之举。 寻求其他援助渠道 加入用户群组或寻求商业支持，理解免费技术支持的局限性。 如何更好地回答问题 友好态度 保持礼貌和善，理解提问者可能承受的压力。 私下回复初犯者 对真诚的新手私下指导，避免公开羞辱。 不确定时明确表态 避免给出错误答案，鼓励提问者提供更多细节。 避免误导性玩笑 不要给出可能破坏提问者设置的玩笑性建议。 引导提问者细化问题 通过反问引导提问者提供更多细节，促进双方学习。 给出高质量答案 避免给出权宜之计，推荐更好的工具或重新定义问题。 正面回答并分享技巧 在回答时强调解决问题的方法，而非单纯提供结果。 推动社区进步 从问题中总结经验教训，改进文档和常见问题解答，以便未来参考。 ","title":"如何向黑客有效地提问"},{"content":"Lilian Weng\u0026rsquo;s blog\n提示工程，也被人说为是上下文学习。它的本质上是用来对齐和激活大模型的能力。它需要大量的实验和启发式方法。\n她提到，迭代的prompt 和 外部工具的使用 没有那么容易被接受。\nBasic Prompting Zero-Shot 就是向LLM简单的提问，向人类交流一样。\nFew-shot 少样本提示包括完整的输入和输出示例，以便大模型理解问题，一般具有更加优秀的回答，能够发挥大模型的能力。但是加入示例会消耗一部分token。\nprompt的格式选择、训练样本、训练样本的顺序都会对结果造成很大的影响。\n有研究调查说明几个有趣的现象：1. 训练数据的label如果分布不均衡，会极大的影响模型能力。2. 最近偏差现象，模型可能会返回训练的最后的几个重复标签，最后训练标签可能权重较大。3. 大模型更加倾向产生常见的token。\nTips for Example Selection 样例选择tips：\n选择一些语义上相似的样例。在embedding层使用k-nn聚类。 为了选择多样的回答，可以使用有向图，通过相邻节点选择数来打分，如果相邻节点选择的多，则打分低，选择几率下降。节点连接通过节点间的embedding cosine相似度判断。 多次采样试验中找出分歧或熵较大的示例。然后对这些示例进行注释，以用于少量提示。 Tips for Example Ordering 选择示例多样性，并且随机排列 上下文示例排列不同，结果不同。增大模型规模或者包含更多样例不能消除这种差距。 Instruction Prompting few-shot会花费token，比价昂贵。直接给出指令形式，可能比较经济。understand and follow。be aligned with human intention。\n注意：1. 需要描述任务非常仔细。specific and precise 具体而精准。\n避免说不要做，而说要做。 解释受众对象。 Self-Consistency Sampling Chain-of-Thought (CoT) 按步骤，短句。解决复杂问题。\nTypes of CoT prompts 主要有两种cot：\nfew-shot cot。示例：高质量推理链。cot包含在示例中，数量：4-8. zero-shot cot。 Let\u0026rsquo;s think step by step. Tips and Extensions sefl-consistency sampling 是在decoder层采样多个答案，并且最终经过投票选择最终答案。 prompt具有较高的推理复杂性可以获得更好的性能。cot分隔推理步骤时，使用换行符的效果最好。 使用复杂示例可以提高解决复杂问题的能力，但对简单问题表现不好。 few-shot，示例：使用Question: \u0026amp; Answer: 效果会更好。 在prompt中包含解释的作用可能为小到中等，非事实性的解释会造成错误的答案。 Automatic Prompt Design Augmented Language Models Retrieval Programming Language External APIs Citation Useful Resources References ","permalink":"https://tanxiangyuu.github.io/posts/llm/prompt_engineering/","summary":"Lilian Weng\u0026rsquo;s blog\n提示工程，也被人说为是上下文学习。它的本质上是用来对齐和激活大模型的能力。它需要大量的实验和启发式方法。\n她提到，迭代的prompt 和 外部工具的使用 没有那么容易被接受。\nBasic Prompting Zero-Shot 就是向LLM简单的提问，向人类交流一样。\nFew-shot 少样本提示包括完整的输入和输出示例，以便大模型理解问题，一般具有更加优秀的回答，能够发挥大模型的能力。但是加入示例会消耗一部分token。\nprompt的格式选择、训练样本、训练样本的顺序都会对结果造成很大的影响。\n有研究调查说明几个有趣的现象：1. 训练数据的label如果分布不均衡，会极大的影响模型能力。2. 最近偏差现象，模型可能会返回训练的最后的几个重复标签，最后训练标签可能权重较大。3. 大模型更加倾向产生常见的token。\nTips for Example Selection 样例选择tips：\n选择一些语义上相似的样例。在embedding层使用k-nn聚类。 为了选择多样的回答，可以使用有向图，通过相邻节点选择数来打分，如果相邻节点选择的多，则打分低，选择几率下降。节点连接通过节点间的embedding cosine相似度判断。 多次采样试验中找出分歧或熵较大的示例。然后对这些示例进行注释，以用于少量提示。 Tips for Example Ordering 选择示例多样性，并且随机排列 上下文示例排列不同，结果不同。增大模型规模或者包含更多样例不能消除这种差距。 Instruction Prompting few-shot会花费token，比价昂贵。直接给出指令形式，可能比较经济。understand and follow。be aligned with human intention。\n注意：1. 需要描述任务非常仔细。specific and precise 具体而精准。\n避免说不要做，而说要做。 解释受众对象。 Self-Consistency Sampling Chain-of-Thought (CoT) 按步骤，短句。解决复杂问题。\nTypes of CoT prompts 主要有两种cot：\nfew-shot cot。示例：高质量推理链。cot包含在示例中，数量：4-8. zero-shot cot。 Let\u0026rsquo;s think step by step. Tips and Extensions sefl-consistency sampling 是在decoder层采样多个答案，并且最终经过投票选择最终答案。 prompt具有较高的推理复杂性可以获得更好的性能。cot分隔推理步骤时，使用换行符的效果最好。 使用复杂示例可以提高解决复杂问题的能力，但对简单问题表现不好。 few-shot，示例：使用Question: \u0026amp; Answer: 效果会更好。 在prompt中包含解释的作用可能为小到中等，非事实性的解释会造成错误的答案。 Automatic Prompt Design Augmented Language Models Retrieval Programming Language External APIs Citation Useful Resources References ","title":"Prompt_engineering"},{"content":"hugo命令 hugo 重建命令：\n1 hugo -F --cleanDestinationDir hugo 新建文档命令：\n1 hugo new dir/name.md git命令 git clone替换命令：\n1 git clone https://github.com/xxx/xxx.git 将url改为：\n1 git clone https://gitclone.com//github.com/xxx/xxx.git ","permalink":"https://tanxiangyuu.github.io/posts/tech/%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E6%96%B9%E5%BC%8F/","summary":"hugo命令 hugo 重建命令：\n1 hugo -F --cleanDestinationDir hugo 新建文档命令：\n1 hugo new dir/name.md git命令 git clone替换命令：\n1 git clone https://github.com/xxx/xxx.git 将url改为：\n1 git clone https://gitclone.com//github.com/xxx/xxx.git ","title":"命令执行方式-个人向"}]