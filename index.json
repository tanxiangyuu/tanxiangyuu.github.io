[{"content":"研究目标 该研究设计和实现了一个能实现多项任务和应用的医学大模型，ClinicalGPT。通过在训练过程中引入多种真实世界数据（如医疗记录、领域特定知识以及多轮对话咨询等）的方式，使其更好地应对多个临床任务。此外，作者还提出了一种全面的评估框架，包括医学知识问答、医学考试、患者咨询以及对电子病历进行诊断分析等多个方面。\n实际问题：\n传统的自然语言处理方法往往需要手动设计特征或规则来进行分类或回答问题，这种方法不仅费时费力，而且难以适应不同场景的需求。而大规模语言模型可以通过无监督学习的方式自动学习到大量的语言规律和语义信息，因此具有更好的泛化能力和适应性。然而，大模型在医疗领域的实际使用中具有局限性。这些模型产生的结果往往与事实不符、逻辑不一致、不连贯，例如引用不存在的文章。它们缺乏真实世界的推理技能和基础，导致了通用和含糊的回复。具体而言，ChatGPT 在医学背景下缺乏深度和洞察力，这可能是由于其基于奖励的训练所使用的排列模型，该模型生成的答案过于笼统，缺乏医学专业知识。\n因此，该研究提出的知识图谱模板化技术和奖励模型可以帮助语言模型更好地理解和应对医学领域中的挑战，从而实现更高效的任务完成。\n是否是新问题：\n虽然在医疗大模型所面临的挑战并非全新，但本研究采用的方法\u0026ndash;开发像 ClinicalGPT 这样的专用模型并针对医疗任务对其进行评估\u0026ndash;却是新颖的。本研究试图直接解决这些已知问题，重点是提高模型在医疗应用中的性能。\n对产业发展的重要意义：\n这项研究对医疗行业的意义在于，它有可能打破特定任务的范式，实现人工智能在医疗保健领域的多功能应用\n新思路与方法 思路\n该研究使用了多种医疗数据集进行模型训练和评估，包括cMedQA2、cMedQA-KG、MD-EHR、MEDQA-MCMLE和MedDialog等。其中：\ncMedQA2是一个包含12万条问题和22.6万条答案的中文医学问答数据集； cMedQA-KG是基于知识图谱构建的医学问答数据集，包含了丰富的医学实体及其关系； MEDQA-MCMLE是基于中国医学考试题库构建的数据集，用于评估临床推理能力； MedDialog是一个由在线平台收集的多轮医学对话数据集，可以模拟医生与患者之间的交互过程； MD-EHR则是一个来自多个大型医院的电子病历数据集，涵盖了各种疾病类型。 为了增强语言模型在特定领域的应用效果，该研究采用了知识图谱模板化技术将知识图谱中的实体和关系转化为适合于指令微调的文本对形式，并利用监督微调的方式进一步优化模型性能。\n此外，该研究还引入了一种奖励模型来为强化学习提供反馈信号，以提高模型生成高质量且有用的输出的概率。\n解决方案关键\n多样的训练数据集 通过知识图谱模板化技术将知识图谱中的实体和关系转化为文本对的形式，可以更好地利用这些结构化的信息来指导模型的学习过程。 同时，奖励模型可以为强化学习提供有效的反馈信号，从而帮助模型生成更加准确和有用的回答。 知识图谱模板化技术如下表所示：\nPrompt Response {疾病}和哪些疾病有关联？ {疾病}可能与{疾病}有关联。 {疾病}可能与其他哪些疾病有关？ {疾病}可能与{疾病}有关联。 {疾病}有哪些常见症状？ {疾病}的常见症状包括{临床表现}。 患有{疾病}的患者可能出现哪些症状？ {疾病}患者可能出现如{临床表现}等症状。 {疾病}的典型临床表现是什么？ {疾病}的典型临床表现为{临床表现}。 患有{疾病}的患者在临床上通常表现为哪些症状？ 患有{疾病}的患者在临床上通常表现为{临床表现}。 诊断{疾病}需要进行哪些检查？ 诊断{疾病}需要进行如{医学检验项目}等检查。 如何检查以确定患有{疾病}？ 确定患有{疾病}需要进行{医学检验项目}等检查。 {药物}主要用于治疗哪些疾病？ {药物}主要用于治疗{疾病}等疾病。 {药物}的适应症是什么？ {药物}的适应症为治疗{疾病}。 如何治疗{疾病}？ 治疗{疾病}的方法包括{医疗程序}。 {疾病}的常见治疗方法有哪些？ {疾病}的常见治疗方法包括{医疗程序}。 {疾病}会引起哪些并发症？ {疾病}会引起{疾病}等并发症。 患有{疾病}的患者可能出现哪些并发症？ 患有{疾病}的患者可能出现{疾病}等并发症。 {药物}与哪些药物存在相互作用？ {药物}与{药物}存在相互作用。 使用{药物}时需要注意哪些药物相互作用？ 使用{药物}时需注意与{药物}的相互作用。 {药物}主要用于治疗哪些症状？ {药物}主要用于治疗{临床表现}等症状。 {药物}的主要治疗作用是什么？ {药物}的主要治疗作用为治疗{临床表现}。 实验设计 本文涉及了以下四个方面的实验：\n医疗对话实验：作者采用了BLEU、ROUGE和GLEU三个评价指标来评估模型生成的对话质量。实验结果表明，ClinicalGPT在所有ROUGE指标上表现优秀，在BLEU-1和大部分ROUGE指标上也表现出色。 医学考试实验：作者选择了MEDQA-MCMLE数据集中的几个类别进行评估，包括医学伦理、呼吸系统、消化系统等。实验结果显示，ClinicalGPT在各个类别中均表现出色，超过了其他LLMs的平均准确率。 诊断实验：作者使用MD-EHR测试集对模型的诊断能力进行了评估。实验结果表明，ClinicalGPT在各个疾病组别中都表现出色，尤其是在消化系统和泌尿系统方面表现最佳。 医学问答实验：作者使用cMedQA2数据集对模型的问答能力进行了评估。实验结果表明，ClinicalGPT在与BLOOM-7B、LLAMA-7B和ChatGLM-6B的比较中表现最好，尤其在与BLOOM-7B和LLAMA-7B的比较中胜出较多。 总的来说，本文的实验结果表明，ClinicalGPT在医疗领域具有出色的应用潜力，特别是在理解和生成医疗对话、医学检查和诊断等方面。然而，在某些特定领域，如妇科和血液系统，可能需要进一步改进。\n论文贡献 多样性数据集构建:训练数据使用了不同来源，覆盖全面的医疗数据\n评估框架构建:建立了包括问答、考试、咨询以及对诊断分析等不同方面的评估标准\n知识图谱模板化:将知识图谱知识转换为 promp 形式，提供更利于理解的训练方式。\n值得探索的问题与挑战 虽然ClinicalGPT在医疗领域的表现已经非常出色，但仍然存在一些挑战需要克服。\n模型准确性提升：如何确保模型输出的准确性、可解释性和安全处理敏感健康数据等问题。因此，在未来的研究中，可以考虑开发更加精细的评估指标和更有效的模型调整方法，以进一步提高模型的性能和适用性。\n医疗场景不足：可以探索将模型应用于更多的医疗场景，如临床决策支持、临床试验招募、临床数据管理和研究支持等领域，为医疗行业带来更大的价值。\n不足和缺失 ClinicalGPT 虽然效果相对其他消费型大模型效果较好，但缺少与闭源模型的比较，并且相关效果并没有达到特别理想的状态。\n学到的内容与启发 可以利用开源数据构建不同场景下的医疗指令，并且使用不同格式的数据源，将其转成可用于微调的指令格式。 数据来源要具备一定的多样性 医疗场景构建也要丰富，意味着指令可以更加丰富，应用可以更加多。 ","permalink":"https://tanxiangyuu.github.io/posts/paper_read/cinicalgpt/","summary":"研究目标 该研究设计和实现了一个能实现多项任务和应用的医学大模型，ClinicalGPT。通过在训练过程中引入多种真实世界数据（如医疗记录、领域特定知识以及多轮对话咨询等）的方式，使其更好地应对多个临床任务。此外，作者还提出了一种全面的评估框架，包括医学知识问答、医学考试、患者咨询以及对电子病历进行诊断分析等多个方面。\n实际问题：\n传统的自然语言处理方法往往需要手动设计特征或规则来进行分类或回答问题，这种方法不仅费时费力，而且难以适应不同场景的需求。而大规模语言模型可以通过无监督学习的方式自动学习到大量的语言规律和语义信息，因此具有更好的泛化能力和适应性。然而，大模型在医疗领域的实际使用中具有局限性。这些模型产生的结果往往与事实不符、逻辑不一致、不连贯，例如引用不存在的文章。它们缺乏真实世界的推理技能和基础，导致了通用和含糊的回复。具体而言，ChatGPT 在医学背景下缺乏深度和洞察力，这可能是由于其基于奖励的训练所使用的排列模型，该模型生成的答案过于笼统，缺乏医学专业知识。\n因此，该研究提出的知识图谱模板化技术和奖励模型可以帮助语言模型更好地理解和应对医学领域中的挑战，从而实现更高效的任务完成。\n是否是新问题：\n虽然在医疗大模型所面临的挑战并非全新，但本研究采用的方法\u0026ndash;开发像 ClinicalGPT 这样的专用模型并针对医疗任务对其进行评估\u0026ndash;却是新颖的。本研究试图直接解决这些已知问题，重点是提高模型在医疗应用中的性能。\n对产业发展的重要意义：\n这项研究对医疗行业的意义在于，它有可能打破特定任务的范式，实现人工智能在医疗保健领域的多功能应用\n新思路与方法 思路\n该研究使用了多种医疗数据集进行模型训练和评估，包括cMedQA2、cMedQA-KG、MD-EHR、MEDQA-MCMLE和MedDialog等。其中：\ncMedQA2是一个包含12万条问题和22.6万条答案的中文医学问答数据集； cMedQA-KG是基于知识图谱构建的医学问答数据集，包含了丰富的医学实体及其关系； MEDQA-MCMLE是基于中国医学考试题库构建的数据集，用于评估临床推理能力； MedDialog是一个由在线平台收集的多轮医学对话数据集，可以模拟医生与患者之间的交互过程； MD-EHR则是一个来自多个大型医院的电子病历数据集，涵盖了各种疾病类型。 为了增强语言模型在特定领域的应用效果，该研究采用了知识图谱模板化技术将知识图谱中的实体和关系转化为适合于指令微调的文本对形式，并利用监督微调的方式进一步优化模型性能。\n此外，该研究还引入了一种奖励模型来为强化学习提供反馈信号，以提高模型生成高质量且有用的输出的概率。\n解决方案关键\n多样的训练数据集 通过知识图谱模板化技术将知识图谱中的实体和关系转化为文本对的形式，可以更好地利用这些结构化的信息来指导模型的学习过程。 同时，奖励模型可以为强化学习提供有效的反馈信号，从而帮助模型生成更加准确和有用的回答。 知识图谱模板化技术如下表所示：\nPrompt Response {疾病}和哪些疾病有关联？ {疾病}可能与{疾病}有关联。 {疾病}可能与其他哪些疾病有关？ {疾病}可能与{疾病}有关联。 {疾病}有哪些常见症状？ {疾病}的常见症状包括{临床表现}。 患有{疾病}的患者可能出现哪些症状？ {疾病}患者可能出现如{临床表现}等症状。 {疾病}的典型临床表现是什么？ {疾病}的典型临床表现为{临床表现}。 患有{疾病}的患者在临床上通常表现为哪些症状？ 患有{疾病}的患者在临床上通常表现为{临床表现}。 诊断{疾病}需要进行哪些检查？ 诊断{疾病}需要进行如{医学检验项目}等检查。 如何检查以确定患有{疾病}？ 确定患有{疾病}需要进行{医学检验项目}等检查。 {药物}主要用于治疗哪些疾病？ {药物}主要用于治疗{疾病}等疾病。 {药物}的适应症是什么？ {药物}的适应症为治疗{疾病}。 如何治疗{疾病}？ 治疗{疾病}的方法包括{医疗程序}。 {疾病}的常见治疗方法有哪些？ {疾病}的常见治疗方法包括{医疗程序}。 {疾病}会引起哪些并发症？ {疾病}会引起{疾病}等并发症。 患有{疾病}的患者可能出现哪些并发症？ 患有{疾病}的患者可能出现{疾病}等并发症。 {药物}与哪些药物存在相互作用？ {药物}与{药物}存在相互作用。 使用{药物}时需要注意哪些药物相互作用？ 使用{药物}时需注意与{药物}的相互作用。 {药物}主要用于治疗哪些症状？ {药物}主要用于治疗{临床表现}等症状。 {药物}的主要治疗作用是什么？ {药物}的主要治疗作用为治疗{临床表现}。 实验设计 本文涉及了以下四个方面的实验：\n医疗对话实验：作者采用了BLEU、ROUGE和GLEU三个评价指标来评估模型生成的对话质量。实验结果表明，ClinicalGPT在所有ROUGE指标上表现优秀，在BLEU-1和大部分ROUGE指标上也表现出色。 医学考试实验：作者选择了MEDQA-MCMLE数据集中的几个类别进行评估，包括医学伦理、呼吸系统、消化系统等。实验结果显示，ClinicalGPT在各个类别中均表现出色，超过了其他LLMs的平均准确率。 诊断实验：作者使用MD-EHR测试集对模型的诊断能力进行了评估。实验结果表明，ClinicalGPT在各个疾病组别中都表现出色，尤其是在消化系统和泌尿系统方面表现最佳。 医学问答实验：作者使用cMedQA2数据集对模型的问答能力进行了评估。实验结果表明，ClinicalGPT在与BLOOM-7B、LLAMA-7B和ChatGLM-6B的比较中表现最好，尤其在与BLOOM-7B和LLAMA-7B的比较中胜出较多。 总的来说，本文的实验结果表明，ClinicalGPT在医疗领域具有出色的应用潜力，特别是在理解和生成医疗对话、医学检查和诊断等方面。然而，在某些特定领域，如妇科和血液系统，可能需要进一步改进。","title":"CLINICALGPT: LARGE LANGUAGE MODELS FINETUNED WITH DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION"},{"content":"1. 怎么下载更快 因为内地特殊原因，从 hf 上下载模式或者数据集速度不堪入目。很多佬都发过提速方法，这里稍微总结一下。\n来源：\n苏洋博客\n如何快速下载huggingface大模型\nhuggingface-cli命令下载。\n1 huggingface-cli download microsoft/phi-2 但是非常不建议使用这个默认命令下载，会很慢，而且会下在根目录的缓存文件中，并且使用 git blob 来保存模型文件，会比原本模型更大。\n可以在这个命令基础上，使用和指定一些参数来完成高效下载。 比如：\n1 huggingface-cli download microsoft/phi-2 --local-dir=./models/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download --local-dir 和下载使用的缓存目录 --cache-dir都设置到了当前目录下，开启 --local-dir-use-symlinks=False 可以让下载的文件后续都以非软链文件保存，方便后续保存或者上传服务器。最后，添加--resume-download 可以确保始终接着之前的下载进度继续，如果下载进度有中断。\nHF_TRANSFER使用\n是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，开启后在带宽充足的机器上可以跑到 500MB/s。 但是有非常明显的缺点：\nHF_TRANSFER不可断点重续!!!具体说明文档 太容易断了！ 启用方法：export HF_HUB_ENABLE_HF_TRANSFER=1\n镜像网站使用\nhttps://hf-mirror.com，设置\n1 export HF_ENDPOINT=\u0026#34;https://hf-mirror.com\u0026#34; 使用摩搭社区下载\n缺点：模型更新有延迟。\n2. 折腾 hf 下载 使用huggingface-cli+HF_TRANSFER下载。\n具体参数：\n--repo-type：下载类型， \u0026lt;model or dataset name\u0026gt;：模型或者数据集名称， --local-dir：本地位置， --cache-dir：缓存位置， --local-dir-use-symlinks：是否使用软链接。 --resume-download：断点续传。 环境设置：\nexport HF_ENDPOINT=\u0026quot;https://hf-mirror.com\u0026quot; export HF_HUB_ENABLE_HF_TRANSFER=1 3. bash 脚本自动配置环境和下载 使用方式示例：\n1 bash hf.sh model baichuan-inc/Baichuan2-7B-Chat ./models/baichuan2-7b-chat ./models/cache 1 下载命令参数组装\n需要传入：--repo-type、\u0026lt;model or dataset name\u0026gt;、--local-dir、--cache-dir、--local-dir-use-symlinks参数标志。\n激活 conda 环境\n（个例需要）进入服务器后，因为环境激活问题，直接使用 huggingface-cli命令下载提示找不到 hf-trans，经过排查，需要重新激活 conda 环境。在issue中提到了一种解决方法，在 conda 安装路径下找到/miniconda3/etc/profile.d/conda.sh，使用source命令运行文件。\n由于打开的 bash 可能已经激活了某个环境，所以先deactivate，再activate。\n配置环境\n在服务器是公用的环境下，直接修改环境变量不方便（就🤡），所以需要改动一下环境的时候，就export一下吧。\n判断是否需要hf_transfer\n用transfer是真快，但是吧，服务器网络总开玩笑，下着下着就莫名其妙断了，这种方式还不能断点重连，是真痛苦。我的服务器实践经验，所下文件如果大于 10g，就不用transfer了，慢点就慢点了，目前 2m 左右的速度也能接受。\n指定本地存储、缓存文件\n公用服务器上，希望文件存储到自己文件夹，就指定这两个文件位置，同时把软链接给取消掉，方便移动模型文件位置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #!/bin/bash source /home/xx/miniconda3/etc/profile.d/conda.sh conda deactivate conda activate base export HF_ENDPOINT=https://hf-mirror.com if [ \u0026#34;$#\u0026#34; -ne 5 ]; then echo \u0026#34;Usage: $0 \u0026lt;type\u0026gt; \u0026lt;name\u0026gt; \u0026lt;local_dir\u0026gt; \u0026lt;cache_dir\u0026gt; \u0026lt;if_hftrans\u0026gt;\u0026#34; exit 1 fi type=$1 name=$2 local_dir=$3 cache_dir=$4 if_hftrans=$5 if [ \u0026#34;$if_hftrans\u0026#34; -eq 1 ]; then export HF_HUB_ENABLE_HF_TRANSFER=1 export huggingface-cli download \\ --repo-type \u0026#34;${type}\u0026#34; \\ \u0026#34;${name}\u0026#34; \\ --local-dir \u0026#34;${local_dir}\u0026#34; \\ --cache-dir \u0026#34;${cache_dir}\u0026#34; \\ --local-dir-use-symlinks False else export huggingface-cli download \\ --repo-type \u0026#34;${type}\u0026#34; \\ \u0026#34;${name}\u0026#34; \\ --local-dir \u0026#34;${local_dir}\u0026#34; \\ --cache-dir \u0026#34;${cache_dir}\u0026#34; \\ --local-dir-use-symlinks False \\ --resume-download fi 4. references Can’t execute conda activate from bash script · Issue #7980 · conda/conda 节省时间：AI 模型靠谱下载方案汇总 如何快速下载huggingface大模型 – padeoe的小站 HF-Mirror https://huggingface.co/docs/huggingface_hub/v0.23.4/package_reference/environment_variables#hfhubenablehftransfer ","permalink":"https://tanxiangyuu.github.io/posts/tech/hf_download/","summary":"1. 怎么下载更快 因为内地特殊原因，从 hf 上下载模式或者数据集速度不堪入目。很多佬都发过提速方法，这里稍微总结一下。\n来源：\n苏洋博客\n如何快速下载huggingface大模型\nhuggingface-cli命令下载。\n1 huggingface-cli download microsoft/phi-2 但是非常不建议使用这个默认命令下载，会很慢，而且会下在根目录的缓存文件中，并且使用 git blob 来保存模型文件，会比原本模型更大。\n可以在这个命令基础上，使用和指定一些参数来完成高效下载。 比如：\n1 huggingface-cli download microsoft/phi-2 --local-dir=./models/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download --local-dir 和下载使用的缓存目录 --cache-dir都设置到了当前目录下，开启 --local-dir-use-symlinks=False 可以让下载的文件后续都以非软链文件保存，方便后续保存或者上传服务器。最后，添加--resume-download 可以确保始终接着之前的下载进度继续，如果下载进度有中断。\nHF_TRANSFER使用\n是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，开启后在带宽充足的机器上可以跑到 500MB/s。 但是有非常明显的缺点：\nHF_TRANSFER不可断点重续!!!具体说明文档 太容易断了！ 启用方法：export HF_HUB_ENABLE_HF_TRANSFER=1\n镜像网站使用\nhttps://hf-mirror.com，设置\n1 export HF_ENDPOINT=\u0026#34;https://hf-mirror.com\u0026#34; 使用摩搭社区下载\n缺点：模型更新有延迟。\n2. 折腾 hf 下载 使用huggingface-cli+HF_TRANSFER下载。\n具体参数：\n--repo-type：下载类型， \u0026lt;model or dataset name\u0026gt;：模型或者数据集名称， --local-dir：本地位置， --cache-dir：缓存位置， --local-dir-use-symlinks：是否使用软链接。 --resume-download：断点续传。 环境设置：\nexport HF_ENDPOINT=\u0026quot;https://hf-mirror.","title":"从 huggingface 下载文件到公共服务器"},{"content":"内网穿透介绍 互联网上两个不同的主机进行通信首先需要知道对方IP。根据IP协议，只有分配了公网IP的设备才能在互联网上通信和传输数据。而中国人口/设备众多，分配到的IPv4资源又少，因此绝大部分情况是通过路由器/交换机转换公网IP后才上网。 位于路由器/交换机后的设备一般是内网设备，分配的IP地址以 192.168/172.16/10.0 开头，属于内网IP。要让内网设备对外提供服务，就需要进行内网穿透。\nfrp介绍 frp 是一个开源、简洁易用、高性能的内网穿透和反向代理软件，支持 tcp, udp, http, https等协议。 frp 项目官网是 GitHub - fatedier/frp，中文官方文档地址：frp/README_zh.md。除了安装过程，中文文档对使用过程已经介绍的非常详细，如遇到问题，建议先查看官方文档。 frp工作原理为：\n服务端运行，监听一个主端口，等待客户端的连接； 客户端连接到服务端的主端口，同时告诉服务端要监听的端口和转发类型； 服务端fork新的进程监听客户端指定的端口； 外网用户连接到客户端指定的端口，服务端通过和客户端的连接将数据转发到客户端； 客户端进程再将数据转发到本地服务，从而实现内网对外暴露服务的能力。 frp内网穿透教程 部署frp服务端（外网机器） 在frp下载界面，下载 frp，注意根据机器类型选择版本，这里因为我的客户端有人已经装了 frp_0.33.0_linux_amd64.tar.gz，所以在服务端也下载这个版本。 解压安装包：tar -zxvf frp_0.33.0_linux_amd64.tar.gz 进入解压目录：cd frp_0.33.0_linux_amd64，编辑 frps.ini文件，编辑完之后删除注释！！！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [common] # frp监听的端口，默认是7000，可以改成其他的 bind_port = 7000 # 授权码，请改成更复杂的 token = 12345678 # frp管理后台端口，请按自己需求更改 dashboard_port = 7500 # frp管理后台用户名和密码，请改成自己的 dashboard_user = admin dashboard_pwd = admin enable_prometheus = true # frp日志配置 log_file = /var/log/frps.log log_level = info log_max_days = 3 设置 frp 服务和开机自启动服务。注意以下 bash 代码需要在进入 frp 文件目录之后完成。 1 2 3 4 5 6 mkdir -p /etc/frp cp frps.ini /etc/frp cp frps /usr/bin cp systemd/frps.service /usr/lib/systemd/system/ systemctl enable frps systemctl start frps 确保监听端口：如 7000，和管理后台端口：如 7500，被服务器放开。 如没有放开，在 centos 系统下，防火墙放行端口： 1 2 3 4 5 # 添加监听端口 firewall-cmd --permanent --add-port=7000/tcp # 添加管理后台端口 firewall-cmd --permanent --add-port=7500/tcp firewall-cmd --reload ubuntu：ufw 设置防火墙。\n浏览器打开“http://服务器IP:后台管理端口” ，输入用户名和密码可以查看连接状态： 配置 frp 客户端（内网服务器），并且适配多个服务端 下载frp下载页面，我这边内网服务器上已经有人下载了 33 版本。 解压缩，进入文件夹内 编辑frpc.ini文件，按照需求转发，因为内网服务器上有人装了 frp 了，所以 frpc.ini文件他已经编辑过了，而一个frpc.ini文件只能设置一个服务端，和多个客户端隧道，这时候不能动人家的文件。只能自己新添加一个frpc1.ini文件，设置好之后添加新的服务。 frpc1.ini文件配置： 1 2 3 4 5 6 7 8 9 10 11 12 [common] server_addr = 服务器ip #设置的端口 server_port = 7000 token = 12345678 #配置客户端ssh服务 [ssh1] # ssh1 服务名，每个服务名字必须唯一，也不能和其他frpc.ini文件里的服务名冲突 type = tcp local_ip = 127.0.0.1 local_port = 22 #内网服务器ssh服务端口，可以设置为其他端口，但请保证其他端口开通了ssh连接权限 remote_port = 8080 #自定义的远程服务器端口，也就是以后通过这个端口进行ssh连接，好像一个伪装一样。 防火墙放行端口 设置 frp 服务，和开机自启动服务。 正常情况下，应该新添加一个服务 **frpc1.service**，为每个 frp 示例创建一个新的独立服务单元，但是我的服务器上，并没有激活 frpc.service 服务，原本用户应该是用nohup直接挂载了 frpc.ini 文件。所以我直接修改了 frpc.service 文件。 需要将frpc.service中的 service 下面重新启动 frpc.ini 文件改为 frpc1.ini文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=Frp Client Service After=network.target [Service] Type=simple User=nobody Restart=on-failure RestartSec=5s ExecStart=/usr/bin/frpc -c /etc/frp/frpc.ini # 改为frpc1.ini ExecReload=/usr/bin/frpc reload -c /etc/frp/frpc.ini # 改为frpc1.ini [Install] WantedBy=multi-user.target 1 2 3 4 5 6 mkdir -p /etc/frp cp frpc1.ini /etc/frp cp frpc /usr/bin cp systemd/frpc.service /usr/lib/systemd/system/ systemctl enable frpc systemctl start frpc 以上命令代码需要在sudo模式下执行。\n登录frp管理后台，观察客户端是否已经连上来。 使用 ssh 命令连接客户端 ssh usr@x.x.x.x -p yyyy\nusr：客户端用户名 x.x.x.x: 服务端 ip yyyy：remote_port 所指端口。 ","permalink":"https://tanxiangyuu.github.io/posts/tech/frp/","summary":"内网穿透介绍 互联网上两个不同的主机进行通信首先需要知道对方IP。根据IP协议，只有分配了公网IP的设备才能在互联网上通信和传输数据。而中国人口/设备众多，分配到的IPv4资源又少，因此绝大部分情况是通过路由器/交换机转换公网IP后才上网。 位于路由器/交换机后的设备一般是内网设备，分配的IP地址以 192.168/172.16/10.0 开头，属于内网IP。要让内网设备对外提供服务，就需要进行内网穿透。\nfrp介绍 frp 是一个开源、简洁易用、高性能的内网穿透和反向代理软件，支持 tcp, udp, http, https等协议。 frp 项目官网是 GitHub - fatedier/frp，中文官方文档地址：frp/README_zh.md。除了安装过程，中文文档对使用过程已经介绍的非常详细，如遇到问题，建议先查看官方文档。 frp工作原理为：\n服务端运行，监听一个主端口，等待客户端的连接； 客户端连接到服务端的主端口，同时告诉服务端要监听的端口和转发类型； 服务端fork新的进程监听客户端指定的端口； 外网用户连接到客户端指定的端口，服务端通过和客户端的连接将数据转发到客户端； 客户端进程再将数据转发到本地服务，从而实现内网对外暴露服务的能力。 frp内网穿透教程 部署frp服务端（外网机器） 在frp下载界面，下载 frp，注意根据机器类型选择版本，这里因为我的客户端有人已经装了 frp_0.33.0_linux_amd64.tar.gz，所以在服务端也下载这个版本。 解压安装包：tar -zxvf frp_0.33.0_linux_amd64.tar.gz 进入解压目录：cd frp_0.33.0_linux_amd64，编辑 frps.ini文件，编辑完之后删除注释！！！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [common] # frp监听的端口，默认是7000，可以改成其他的 bind_port = 7000 # 授权码，请改成更复杂的 token = 12345678 # frp管理后台端口，请按自己需求更改 dashboard_port = 7500 # frp管理后台用户名和密码，请改成自己的 dashboard_user = admin dashboard_pwd = admin enable_prometheus = true # frp日志配置 log_file = /var/log/frps.","title":"Frp"},{"content":"对于堆来说，有几个重要的点，虽然简单，但是需要理解，才能理解堆\n完全二叉树 堆是一个完全二叉树 可以分成，大顶堆和小顶堆 大顶堆：父节点大于等于子节点 小顶堆：父节点小于等于子节点 堆的存储：数组形式\u0026ndash;利用完全二叉树的性质。 ","permalink":"https://tanxiangyuu.github.io/posts/algo/%E5%A0%86%E6%8E%92%E5%BA%8F/","summary":"对于堆来说，有几个重要的点，虽然简单，但是需要理解，才能理解堆\n完全二叉树 堆是一个完全二叉树 可以分成，大顶堆和小顶堆 大顶堆：父节点大于等于子节点 小顶堆：父节点小于等于子节点 堆的存储：数组形式\u0026ndash;利用完全二叉树的性质。 ","title":"堆排序"},{"content":"题目：假设有一组无序的数字，找到其中排名第 k 位的数字。\n快排 快速排序是一种优秀的排序算法。\nC++ STL 的 sort，使用的就是“快速排序 + 插入排序 + 堆排序”的方式。\n快排的核心 找准基准值的位置\n通过的是partition操作，将数组分为两部分，小于基准值的放在左边，大于基准值的放在右边。\n然后通过递归，对两边继续进行partition操作。\n问题\n求排名为k的元素，并不需要对整个数组进行排序。 时间复杂度不稳定，最坏的情况会达到O(n^2)。 快速选择排序 针对第一个问题：使用快速选择排序方法解决。在不对数据整体进行排序的前提下，快速找到排名第 k 位的元素，而且时间复杂度还能优化到 O(n)。\n快速选择的核心 当需要快速找到一个元素 X，并且使得小于 X 的元素数量是 k-1 个时，那 X 就是要查找的排名第 k 位的元素了。\n没有必要对整个数组进行排序。\n依旧使用partition操作进行实现。 在partition操作中，将基准值排名ind和 k 进行比较。\n如果 ind 正好等于 k，那说明当前的基准值，就是要找的排名第 k 位的元素； 如果 ind 大于 k，说明排名第 k 位的元素在基准值的前面。接下来，要解决的问题就是，在基准值的前面查找排名第 k 位的元素； 如果 ind 小于 k ，就说明排名第 k 位的元素在基准值的后面，并且，当前包括基准值在内的 ind 个元素，都是小于基准值的元素。那么，问题就转化成了，在基准值的后面查找排名第 k - ind 位的元素。 三种快排的优化 针对第二个问题，稳定系统运行时间。\n三种方法 优化1：单边递归优化 在快排的实现中，使用的是双边递归。如代码：\n1 2 quick_sort(arr, l, x - 1); // 对左半边排序 quick_sort(arr, x + 1 , r); // 对右半边排序 从程序的运行时间来考虑的话，每次函数调用，都会消耗掉一部分运行时间。那只要可以减少函数调用的次数，其实就可以加快一点程序运行的速度。\n单边递归：代码：\n1 2 3 4 5 6 7 8 9 10 11 12 void quick_sort(int *arr, int l, int r) { while (l \u0026lt; r) { // 进行一轮 partition 操作 // 获得基准值的位置 int ind = partition(arr, l, r); // 右侧正常调用递归函数 quick_sort(arr, ind + 1, r); // 用本层处理左侧的排序 r = ind - 1; } return ; } 优化2：三数取中优化-基准值选取优化 基准值选取不合理，会导致算法效率的降低。只有当基准值每次都能将排序区间中的数据平分时，时间复杂度才是最好情况下的 O(nlogn)。\n所谓三点取中法，就是每一轮取排序区间的头、尾和中间元素这三个值，然后把它们排序以后的中间值作为本轮的基准值。\n优化3：partition 操作优化 核心思想：头指针找小值，尾指针找大值，然后交换。免去填空过程。\n补充 在运用快速选择算法以寻找排名第 k 位置的元素时，实际上，一旦我们借助该算法确定了第 k 位元素的值，将这个值与它之前的元素值累加起来，即可得到前 k 位元素的所有值。换言之，快速选择算法不仅能有效解决寻找单个第 k 位元素的问题，还能进一步应用于解决求解前 k 个小或前 k 个大元素等更为广泛的 Top-K 问题。\n","permalink":"https://tanxiangyuu.github.io/posts/algo/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/","summary":"题目：假设有一组无序的数字，找到其中排名第 k 位的数字。\n快排 快速排序是一种优秀的排序算法。\nC++ STL 的 sort，使用的就是“快速排序 + 插入排序 + 堆排序”的方式。\n快排的核心 找准基准值的位置\n通过的是partition操作，将数组分为两部分，小于基准值的放在左边，大于基准值的放在右边。\n然后通过递归，对两边继续进行partition操作。\n问题\n求排名为k的元素，并不需要对整个数组进行排序。 时间复杂度不稳定，最坏的情况会达到O(n^2)。 快速选择排序 针对第一个问题：使用快速选择排序方法解决。在不对数据整体进行排序的前提下，快速找到排名第 k 位的元素，而且时间复杂度还能优化到 O(n)。\n快速选择的核心 当需要快速找到一个元素 X，并且使得小于 X 的元素数量是 k-1 个时，那 X 就是要查找的排名第 k 位的元素了。\n没有必要对整个数组进行排序。\n依旧使用partition操作进行实现。 在partition操作中，将基准值排名ind和 k 进行比较。\n如果 ind 正好等于 k，那说明当前的基准值，就是要找的排名第 k 位的元素； 如果 ind 大于 k，说明排名第 k 位的元素在基准值的前面。接下来，要解决的问题就是，在基准值的前面查找排名第 k 位的元素； 如果 ind 小于 k ，就说明排名第 k 位的元素在基准值的后面，并且，当前包括基准值在内的 ind 个元素，都是小于基准值的元素。那么，问题就转化成了，在基准值的后面查找排名第 k - ind 位的元素。 三种快排的优化 针对第二个问题，稳定系统运行时间。","title":"快速选择排序"},{"content":"RAG-Retrieval Augmented Generation 1. RAG介绍 RAG，检索增强生成技术，是一种基于检索的生成模型，它结合了生成模型的灵活性和检索模型的效率。RAG通过将生成模型与检索模型相结合，实现了高效的文本生成。\n目的：解决LLM的幻觉情况，加深专业领域深度，更新知识库。\n最小RAG的基本结构：\n向量化模块，将文档（文本）片段向量化。 文档加载和切分模块，加载文档和切分文档片段。 数据库模块，存放切分好的文档片段及其对应的向量表示 检索模块，根据query检索相应的文档片段。 大模型模块，根据检索出的文档来回答query 生成answer。 RAG的流程 ：\n索引：将文档库分割成较短的chunk，并且通过编码器构建向量索引。 检索：根据query和chunks的相似度检索相关文档片段。 生成：通过检索到的文档，作为上下文条件，生成answer。 2. 各个模块的code demo code 待补充。。。\n3. 总结 一个最小的RAG包括：\n向量化模块 文档加载和切分模块 数据库 检索模块–向量 大模型模块 4. 论文 [1] RAG: Retrieval Augmented Generation for Dense Text-to-Text Pre-training. ","permalink":"https://tanxiangyuu.github.io/posts/llm/rag/","summary":"RAG-Retrieval Augmented Generation 1. RAG介绍 RAG，检索增强生成技术，是一种基于检索的生成模型，它结合了生成模型的灵活性和检索模型的效率。RAG通过将生成模型与检索模型相结合，实现了高效的文本生成。\n目的：解决LLM的幻觉情况，加深专业领域深度，更新知识库。\n最小RAG的基本结构：\n向量化模块，将文档（文本）片段向量化。 文档加载和切分模块，加载文档和切分文档片段。 数据库模块，存放切分好的文档片段及其对应的向量表示 检索模块，根据query检索相应的文档片段。 大模型模块，根据检索出的文档来回答query 生成answer。 RAG的流程 ：\n索引：将文档库分割成较短的chunk，并且通过编码器构建向量索引。 检索：根据query和chunks的相似度检索相关文档片段。 生成：通过检索到的文档，作为上下文条件，生成answer。 2. 各个模块的code demo code 待补充。。。\n3. 总结 一个最小的RAG包括：\n向量化模块 文档加载和切分模块 数据库 检索模块–向量 大模型模块 4. 论文 [1] RAG: Retrieval Augmented Generation for Dense Text-to-Text Pre-training. ","title":"RAG"},{"content":"本文是针对PyTorch中的张量（Tensors）的一个基础教程，它详细介绍了张量的定义、特性、以及如何在PyTorch中使用张量进行基本操作。张量是PyTorch中进行科学计算的基础，它们可以视为一个高维数组或矩阵。本教程的主要内容包括：\n张量初始化 从数据直接创建张量：可以直接从数据创建张量，PyTorch会自动推断数据类型。 从NumPy数组创建：可以使用torch.from_numpy()从NumPy数组创建张量。 通过已有的张量创建：可以通过已有的张量来创建新的张量。这种方法会默认重用输入张量的属性（如数据类型），除非显式地进行更改。 使用随机或常数值：torch.rand()创建随机初始化的张量，torch.zeros()和torch.ones()分别创建全0或全1的张量。 张量属性 张量属性：张量的属性包括形状（shape）、数据类型（dtype）和存储的设备（device，如CPU或GPU）。 张量操作 索引和切片：可以使用标准的Python索引和切片操作来访问张量的部分。 张量重塑：reshape可以改变张量的形状而不改变其数据。 张量合并：torch.cat可以用来在给定维度上合并张量序列。 张量乘法：介绍了元素乘法（*或torch.mul）和矩阵乘法（@或torch.matmul）。 张量与NumPy之间的转换 张量可以很容易地与NumPy数组相互转换，使用numpy()方法从张量转换为NumPy数组，使用torch.from_numpy()从NumPy数组转换为张量。这两种类型的转换是共享底层内存的，因此修改其中一个会影响另一个。 自动微分 自动微分：PyTorch中的自动微分是通过autograd模块实现的，它提供了张量上所有操作的自动微分。这对于深度学习训练中的梯度计算非常有用。 运行在GPU上 张量可以被移到GPU上：使用.to方法可以将张量移动到任何设备上，这对于加速计算非常重要。 本教程适合初学者，通过详细的示例和解释，帮助读者理解和掌握如何在PyTorch中有效地使用张量进行数据操作和计算。\n","permalink":"https://tanxiangyuu.github.io/posts/llm/pytorch/","summary":"本文是针对PyTorch中的张量（Tensors）的一个基础教程，它详细介绍了张量的定义、特性、以及如何在PyTorch中使用张量进行基本操作。张量是PyTorch中进行科学计算的基础，它们可以视为一个高维数组或矩阵。本教程的主要内容包括：\n张量初始化 从数据直接创建张量：可以直接从数据创建张量，PyTorch会自动推断数据类型。 从NumPy数组创建：可以使用torch.from_numpy()从NumPy数组创建张量。 通过已有的张量创建：可以通过已有的张量来创建新的张量。这种方法会默认重用输入张量的属性（如数据类型），除非显式地进行更改。 使用随机或常数值：torch.rand()创建随机初始化的张量，torch.zeros()和torch.ones()分别创建全0或全1的张量。 张量属性 张量属性：张量的属性包括形状（shape）、数据类型（dtype）和存储的设备（device，如CPU或GPU）。 张量操作 索引和切片：可以使用标准的Python索引和切片操作来访问张量的部分。 张量重塑：reshape可以改变张量的形状而不改变其数据。 张量合并：torch.cat可以用来在给定维度上合并张量序列。 张量乘法：介绍了元素乘法（*或torch.mul）和矩阵乘法（@或torch.matmul）。 张量与NumPy之间的转换 张量可以很容易地与NumPy数组相互转换，使用numpy()方法从张量转换为NumPy数组，使用torch.from_numpy()从NumPy数组转换为张量。这两种类型的转换是共享底层内存的，因此修改其中一个会影响另一个。 自动微分 自动微分：PyTorch中的自动微分是通过autograd模块实现的，它提供了张量上所有操作的自动微分。这对于深度学习训练中的梯度计算非常有用。 运行在GPU上 张量可以被移到GPU上：使用.to方法可以将张量移动到任何设备上，这对于加速计算非常重要。 本教程适合初学者，通过详细的示例和解释，帮助读者理解和掌握如何在PyTorch中有效地使用张量进行数据操作和计算。","title":"PyTorch张量基础教程总结"},{"content":"原文链接\n引言 尊重与期待：黑客欣赏高质量问题，对好问题持积极态度，但对不假思索或懒惰提问者有所抵触。 提问前的准备工作 自行寻找答案 搜索论坛帖子 使用搜索引擎查询 阅读手册与常见问题解答(FAQ) 自行尝试和测试 向身边专家咨询 如是程序员，阅读源代码 展示前期努力：在提问时，告知你已经尝试过的解决途径，证明你不是一个坐享其成者。 提问技巧 选择合适的提问场所：避免在不相关的论坛提问，查找并选用专门针对你问题的主题论坛或邮件列表。 利用Stack Overflow和Stack Exchange：在提问前先搜索，然后在对应主题的Stack Exchange网站上提问。 邮件列表与IRC：参加项目邮件列表，若有必要，可在开发者邮件列表提问，前提是你已经尝试过用户列表。 编写有效的标题： 标题示例：目标——差异式描述 示例：X.org 6.8.1鼠标指针在MV1005显卡芯片组下的变形问题 方便回复：不要请求直接回复到个人邮箱，而是使用恰当的论坛或邮件列表提问，并确保标题能准确反映问题内容。 清晰表述： 使用正确、清晰、准确的语言，避免拼写、语法错误。 尊重非母语使用者，但力求表达清晰，必要时声明语言障碍。 便于阅读的格式： 使用纯文本格式，避免HTML和特殊格式。 不要发送封闭格式的文档，如Word或Excel文件。 在论坛中适度使用表情符号和格式，保持专业简洁。 问题描述详尽： 描述问题的具体表现。 提供环境信息，如操作系统、应用版本等。 描述已经进行的研究与诊断步骤。 提供可能导致问题变化的背景信息。 提供重现问题的简明步骤。 精炼问题：提供简明扼要且具有针对性的信息，尽量缩小问题范围。 谨慎声称发现Bug： 在确定是Bug之前，先假设可能自身操作不当。 提供详尽的重现步骤及可能的补丁或回归测试证据。 低声下气无助于解决问题 避免以过分谦卑或模糊的方式提出问题，应清晰表述背景及问题细节，不必自我贬低。明确问题的具体症状而非主观猜测，以便他人准确判断和诊断。 提供精确的技术信息 当提问关于技术问题时，务必提供详尽的硬件配置、软件环境及错误现象等信息，如编译错误的例子所示，确保对方能看到与你相同的现象，而非仅听信个人揣测。 遵循问题发生的时间顺序 记录问题出现前的操作流程，按时间顺序详细描述，包括系统的反应和出现问题的具体环节。如有必要，提供调试日志和相关设置，确保信息具有针对性和实用性。 聚焦问题实质而非操作过程 提问时首先阐明你的最终目标，而非纠结于某一无效步骤。例如，当询问颜色选择器获取RGB值时，应说明实际需要达成的任务，以便得到更有效的解决方案。 鼓励公开交流 避免请求私下回复，提倡在公共平台讨论问题，以便更多人参与、纠正错误，并为解答者带来社区认可。只有在特殊情况下，如预期收到大量重复回复时，可提议私下整合信息后回馈至公共平台。 清晰简洁地表述需求 明确告知需要何种形式的帮助，限制所需投入的时间和精力，使专家更容易给出有针对性的回答。简化问题表述，比如求教参考资料来源优于直接要求解释概念。 提供最小化复现代码 当涉及编程问题时，附上精简且能够复现问题的代码片段，明确指出期望结果和实际结果之间的差异。创建最精简的测试用例有助于他人快速定位问题，从而提高获得有效回复的可能性。 对待家庭作业的态度 对于疑似家庭作业性质的问题，黑客社群鼓励自行解决以积累经验。可以请求提示，但不应索要完整答案。在适当场合如用户论坛中提问时，表明你已尽力但仍需指导，或许能得到有益的提示。 去除无意义提问以提升问题质量 避免使用无助于解决问题的结尾疑问句，如\u0026quot;有人能帮我吗？\u0026quot; 不明确的问题加上这类问句反而显多余，易遭黑客社区反感 尽量减少\u0026quot;是/否\u0026quot;型提问，除非期望得到确切的二元答复 慎用“紧急”标签以获取有效关注 标注“紧急”往往适得其反，可能导致问题被忽视或删除 在非专业场合高调标榜紧急可能因垃圾信息过滤而失效 特殊情况下的紧急提及须礼貌且基于共同兴趣点 礼仪助力沟通，提升获得解答的可能性 表达感谢，如使用“请”和“谢谢您的关注”，展示对他人力气的认可 礼貌虽非首要，却有益于问题得到关注，尤其在连续提问情境下 先致谢后再次感谢回复者的具体帮助，避免误解 问题解决后的跟进与分享 解决问题后及时告知并感谢参与者 使用“已解决”等标记更新原话题标题，方便他人查阅 简洁概述问题解决方法及原因，提及关键协助者 提供简洁的调试摘要或指出避免问题的途径 正确反馈问题解决状态有助于满足解答者成就感 考虑编写文档或添加FAQ，防止他人重复遇到相同问题 如何解读答案 RTFM 和 STFW：遭遇经典回应的意义与应对 当收到RTFM或STFW回复时，这意味着你应该查阅相关手册或自行上网搜索。 这类回答表明信息易于获取，自行搜索能促进学习，而非直接给予答案。 收到此类回复应视为对方已在某种程度上提供了关注，对此表示感谢。 面对困惑时的正确求解姿态 若无法理解回复，尝试自行解决，参考手册、FAQ、网络资源或请教他人。 提出疑问时展现自主探索过程，如举例询问具体的细节而非基础概念。 应对无礼回应与黑客文化 黑客圈内的直率交流可能被误认为无礼，遇此情况保持冷静。 如确实遭受冒犯，社群内其他成员可能会介入纠正不当行为。 对于真正的冒犯者，有力反击可被接受，但新手需谨慎判断，避免陷入无谓争执。 黑客文化中的某些特质可能源于独特的社交习惯，不必过分解读。 Jeff Bigler 的观点：应对策略 Jeff Bigler 关于“社交过滤器”的观察可作为理解和适应黑客社群互动方式的参考。 如何避免扮演失败者 面对批评与纠正 接受并忍受在黑客社区中因失误而受到的公开批评，这是社区标准得以维持的方式。不应期待所有意见都通过私人途径传达，避免将建设性批评视为个人攻击。 处理挑衅行为 遇到无端指责时，保持冷静，避免陷入口水战。黑客指出错误是出于关心社区和个人成长，不应为此抱怨或要求特殊对待。 学会区分口水战与实质性回复 多数口水战无需理会，确认其中是否包含对问题实质的解答或有价值的建议。 不该问的问题及其典型回应 寻找资源路径 利用搜索引擎寻找所需程序或资源，基本查询能力应当具备。 操作方法问题 提问过于宽泛，未能明确问题焦点，建议先自行研究Y问题的本质。 配置问题 自行阅读手册（RTFM），自行查找答案。 文件转换问题 自行尝试并验证可行性。 无效的程序/设置问题表述 提供具体问题详情，避免空泛陈述。 针对Windows的问题 优先考虑更换操作系统或在适当场合提问，如涉及与开源软件交互问题。 质疑系统工具有效性 确保问题归因准确，提供详细证据。 安装Linux或其他操作系统问题 寻求本地用户群组的帮助，提供具体故障细节。 非法活动请求 黑客不会支持非法行为，此类问题不会得到回答。 好问题与蠢问题的区别 展示搜索努力 聪明的问题表明提问者已经尽力搜索但仍未解决问题，寻求进一步指引。 尊重他人时间 避免责备他人或表现出傲慢态度，详细描述问题背景和已尝试的解决方案。 得不到回答时的对策 保持耐心 问题可能因多种原因暂时未得到回答，重复张贴问题非明智之举。 寻求其他援助渠道 加入用户群组或寻求商业支持，理解免费技术支持的局限性。 如何更好地回答问题 友好态度 保持礼貌和善，理解提问者可能承受的压力。 私下回复初犯者 对真诚的新手私下指导，避免公开羞辱。 不确定时明确表态 避免给出错误答案，鼓励提问者提供更多细节。 避免误导性玩笑 不要给出可能破坏提问者设置的玩笑性建议。 引导提问者细化问题 通过反问引导提问者提供更多细节，促进双方学习。 给出高质量答案 避免给出权宜之计，推荐更好的工具或重新定义问题。 正面回答并分享技巧 在回答时强调解决问题的方法，而非单纯提供结果。 推动社区进步 从问题中总结经验教训，改进文档和常见问题解答，以便未来参考。 ","permalink":"https://tanxiangyuu.github.io/posts/tech/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/","summary":"原文链接\n引言 尊重与期待：黑客欣赏高质量问题，对好问题持积极态度，但对不假思索或懒惰提问者有所抵触。 提问前的准备工作 自行寻找答案 搜索论坛帖子 使用搜索引擎查询 阅读手册与常见问题解答(FAQ) 自行尝试和测试 向身边专家咨询 如是程序员，阅读源代码 展示前期努力：在提问时，告知你已经尝试过的解决途径，证明你不是一个坐享其成者。 提问技巧 选择合适的提问场所：避免在不相关的论坛提问，查找并选用专门针对你问题的主题论坛或邮件列表。 利用Stack Overflow和Stack Exchange：在提问前先搜索，然后在对应主题的Stack Exchange网站上提问。 邮件列表与IRC：参加项目邮件列表，若有必要，可在开发者邮件列表提问，前提是你已经尝试过用户列表。 编写有效的标题： 标题示例：目标——差异式描述 示例：X.org 6.8.1鼠标指针在MV1005显卡芯片组下的变形问题 方便回复：不要请求直接回复到个人邮箱，而是使用恰当的论坛或邮件列表提问，并确保标题能准确反映问题内容。 清晰表述： 使用正确、清晰、准确的语言，避免拼写、语法错误。 尊重非母语使用者，但力求表达清晰，必要时声明语言障碍。 便于阅读的格式： 使用纯文本格式，避免HTML和特殊格式。 不要发送封闭格式的文档，如Word或Excel文件。 在论坛中适度使用表情符号和格式，保持专业简洁。 问题描述详尽： 描述问题的具体表现。 提供环境信息，如操作系统、应用版本等。 描述已经进行的研究与诊断步骤。 提供可能导致问题变化的背景信息。 提供重现问题的简明步骤。 精炼问题：提供简明扼要且具有针对性的信息，尽量缩小问题范围。 谨慎声称发现Bug： 在确定是Bug之前，先假设可能自身操作不当。 提供详尽的重现步骤及可能的补丁或回归测试证据。 低声下气无助于解决问题 避免以过分谦卑或模糊的方式提出问题，应清晰表述背景及问题细节，不必自我贬低。明确问题的具体症状而非主观猜测，以便他人准确判断和诊断。 提供精确的技术信息 当提问关于技术问题时，务必提供详尽的硬件配置、软件环境及错误现象等信息，如编译错误的例子所示，确保对方能看到与你相同的现象，而非仅听信个人揣测。 遵循问题发生的时间顺序 记录问题出现前的操作流程，按时间顺序详细描述，包括系统的反应和出现问题的具体环节。如有必要，提供调试日志和相关设置，确保信息具有针对性和实用性。 聚焦问题实质而非操作过程 提问时首先阐明你的最终目标，而非纠结于某一无效步骤。例如，当询问颜色选择器获取RGB值时，应说明实际需要达成的任务，以便得到更有效的解决方案。 鼓励公开交流 避免请求私下回复，提倡在公共平台讨论问题，以便更多人参与、纠正错误，并为解答者带来社区认可。只有在特殊情况下，如预期收到大量重复回复时，可提议私下整合信息后回馈至公共平台。 清晰简洁地表述需求 明确告知需要何种形式的帮助，限制所需投入的时间和精力，使专家更容易给出有针对性的回答。简化问题表述，比如求教参考资料来源优于直接要求解释概念。 提供最小化复现代码 当涉及编程问题时，附上精简且能够复现问题的代码片段，明确指出期望结果和实际结果之间的差异。创建最精简的测试用例有助于他人快速定位问题，从而提高获得有效回复的可能性。 对待家庭作业的态度 对于疑似家庭作业性质的问题，黑客社群鼓励自行解决以积累经验。可以请求提示，但不应索要完整答案。在适当场合如用户论坛中提问时，表明你已尽力但仍需指导，或许能得到有益的提示。 去除无意义提问以提升问题质量 避免使用无助于解决问题的结尾疑问句，如\u0026quot;有人能帮我吗？\u0026quot; 不明确的问题加上这类问句反而显多余，易遭黑客社区反感 尽量减少\u0026quot;是/否\u0026quot;型提问，除非期望得到确切的二元答复 慎用“紧急”标签以获取有效关注 标注“紧急”往往适得其反，可能导致问题被忽视或删除 在非专业场合高调标榜紧急可能因垃圾信息过滤而失效 特殊情况下的紧急提及须礼貌且基于共同兴趣点 礼仪助力沟通，提升获得解答的可能性 表达感谢，如使用“请”和“谢谢您的关注”，展示对他人力气的认可 礼貌虽非首要，却有益于问题得到关注，尤其在连续提问情境下 先致谢后再次感谢回复者的具体帮助，避免误解 问题解决后的跟进与分享 解决问题后及时告知并感谢参与者 使用“已解决”等标记更新原话题标题，方便他人查阅 简洁概述问题解决方法及原因，提及关键协助者 提供简洁的调试摘要或指出避免问题的途径 正确反馈问题解决状态有助于满足解答者成就感 考虑编写文档或添加FAQ，防止他人重复遇到相同问题 如何解读答案 RTFM 和 STFW：遭遇经典回应的意义与应对 当收到RTFM或STFW回复时，这意味着你应该查阅相关手册或自行上网搜索。 这类回答表明信息易于获取，自行搜索能促进学习，而非直接给予答案。 收到此类回复应视为对方已在某种程度上提供了关注，对此表示感谢。 面对困惑时的正确求解姿态 若无法理解回复，尝试自行解决，参考手册、FAQ、网络资源或请教他人。 提出疑问时展现自主探索过程，如举例询问具体的细节而非基础概念。 应对无礼回应与黑客文化 黑客圈内的直率交流可能被误认为无礼，遇此情况保持冷静。 如确实遭受冒犯，社群内其他成员可能会介入纠正不当行为。 对于真正的冒犯者，有力反击可被接受，但新手需谨慎判断，避免陷入无谓争执。 黑客文化中的某些特质可能源于独特的社交习惯，不必过分解读。 Jeff Bigler 的观点：应对策略 Jeff Bigler 关于“社交过滤器”的观察可作为理解和适应黑客社群互动方式的参考。 如何避免扮演失败者 面对批评与纠正 接受并忍受在黑客社区中因失误而受到的公开批评，这是社区标准得以维持的方式。不应期待所有意见都通过私人途径传达，避免将建设性批评视为个人攻击。 处理挑衅行为 遇到无端指责时，保持冷静，避免陷入口水战。黑客指出错误是出于关心社区和个人成长，不应为此抱怨或要求特殊对待。 学会区分口水战与实质性回复 多数口水战无需理会，确认其中是否包含对问题实质的解答或有价值的建议。 不该问的问题及其典型回应 寻找资源路径 利用搜索引擎寻找所需程序或资源，基本查询能力应当具备。 操作方法问题 提问过于宽泛，未能明确问题焦点，建议先自行研究Y问题的本质。 配置问题 自行阅读手册（RTFM），自行查找答案。 文件转换问题 自行尝试并验证可行性。 无效的程序/设置问题表述 提供具体问题详情，避免空泛陈述。 针对Windows的问题 优先考虑更换操作系统或在适当场合提问，如涉及与开源软件交互问题。 质疑系统工具有效性 确保问题归因准确，提供详细证据。 安装Linux或其他操作系统问题 寻求本地用户群组的帮助，提供具体故障细节。 非法活动请求 黑客不会支持非法行为，此类问题不会得到回答。 好问题与蠢问题的区别 展示搜索努力 聪明的问题表明提问者已经尽力搜索但仍未解决问题，寻求进一步指引。 尊重他人时间 避免责备他人或表现出傲慢态度，详细描述问题背景和已尝试的解决方案。 得不到回答时的对策 保持耐心 问题可能因多种原因暂时未得到回答，重复张贴问题非明智之举。 寻求其他援助渠道 加入用户群组或寻求商业支持，理解免费技术支持的局限性。 如何更好地回答问题 友好态度 保持礼貌和善，理解提问者可能承受的压力。 私下回复初犯者 对真诚的新手私下指导，避免公开羞辱。 不确定时明确表态 避免给出错误答案，鼓励提问者提供更多细节。 避免误导性玩笑 不要给出可能破坏提问者设置的玩笑性建议。 引导提问者细化问题 通过反问引导提问者提供更多细节，促进双方学习。 给出高质量答案 避免给出权宜之计，推荐更好的工具或重新定义问题。 正面回答并分享技巧 在回答时强调解决问题的方法，而非单纯提供结果。 推动社区进步 从问题中总结经验教训，改进文档和常见问题解答，以便未来参考。 ","title":"如何向黑客有效地提问"},{"content":"Lilian Weng\u0026rsquo;s blog\n提示工程，也被人说为是上下文学习。它的本质上是用来对齐和激活大模型的能力。它需要大量的实验和启发式方法。\n她提到，迭代的prompt 和 外部工具的使用 没有那么容易被接受。\nBasic Prompting Zero-Shot 就是向LLM简单的提问，向人类交流一样。\nFew-shot 少样本提示包括完整的输入和输出示例，以便大模型理解问题，一般具有更加优秀的回答，能够发挥大模型的能力。但是加入示例会消耗一部分token。\nprompt的格式选择、训练样本、训练样本的顺序都会对结果造成很大的影响。\n有研究调查说明几个有趣的现象：1. 训练数据的label如果分布不均衡，会极大的影响模型能力。2. 最近偏差现象，模型可能会返回训练的最后的几个重复标签，最后训练标签可能权重较大。3. 大模型更加倾向产生常见的token。\nTips for Example Selection 样例选择tips：\n选择一些语义上相似的样例。在embedding层使用k-nn聚类。 为了选择多样的回答，可以使用有向图，通过相邻节点选择数来打分，如果相邻节点选择的多，则打分低，选择几率下降。节点连接通过节点间的embedding cosine相似度判断。 多次采样试验中找出分歧或熵较大的示例。然后对这些示例进行注释，以用于少量提示。 Tips for Example Ordering 选择示例多样性，并且随机排列 上下文示例排列不同，结果不同。增大模型规模或者包含更多样例不能消除这种差距。 Instruction Prompting few-shot会花费token，比价昂贵。直接给出指令形式，可能比较经济。understand and follow。be aligned with human intention。\n注意：1. 需要描述任务非常仔细。specific and precise 具体而精准。\n避免说不要做，而说要做。 解释受众对象。 Self-Consistency Sampling Chain-of-Thought (CoT) 按步骤，短句。解决复杂问题。\nTypes of CoT prompts 主要有两种cot：\nfew-shot cot。示例：高质量推理链。cot包含在示例中，数量：4-8. zero-shot cot。 Let\u0026rsquo;s think step by step. Tips and Extensions sefl-consistency sampling 是在decoder层采样多个答案，并且最终经过投票选择最终答案。 prompt具有较高的推理复杂性可以获得更好的性能。cot分隔推理步骤时，使用换行符的效果最好。 使用复杂示例可以提高解决复杂问题的能力，但对简单问题表现不好。 few-shot，示例：使用Question: \u0026amp; Answer: 效果会更好。 在prompt中包含解释的作用可能为小到中等，非事实性的解释会造成错误的答案。 Automatic Prompt Design Augmented Language Models Retrieval Programming Language External APIs Citation Useful Resources References ","permalink":"https://tanxiangyuu.github.io/posts/llm/prompt_engineering/","summary":"Lilian Weng\u0026rsquo;s blog\n提示工程，也被人说为是上下文学习。它的本质上是用来对齐和激活大模型的能力。它需要大量的实验和启发式方法。\n她提到，迭代的prompt 和 外部工具的使用 没有那么容易被接受。\nBasic Prompting Zero-Shot 就是向LLM简单的提问，向人类交流一样。\nFew-shot 少样本提示包括完整的输入和输出示例，以便大模型理解问题，一般具有更加优秀的回答，能够发挥大模型的能力。但是加入示例会消耗一部分token。\nprompt的格式选择、训练样本、训练样本的顺序都会对结果造成很大的影响。\n有研究调查说明几个有趣的现象：1. 训练数据的label如果分布不均衡，会极大的影响模型能力。2. 最近偏差现象，模型可能会返回训练的最后的几个重复标签，最后训练标签可能权重较大。3. 大模型更加倾向产生常见的token。\nTips for Example Selection 样例选择tips：\n选择一些语义上相似的样例。在embedding层使用k-nn聚类。 为了选择多样的回答，可以使用有向图，通过相邻节点选择数来打分，如果相邻节点选择的多，则打分低，选择几率下降。节点连接通过节点间的embedding cosine相似度判断。 多次采样试验中找出分歧或熵较大的示例。然后对这些示例进行注释，以用于少量提示。 Tips for Example Ordering 选择示例多样性，并且随机排列 上下文示例排列不同，结果不同。增大模型规模或者包含更多样例不能消除这种差距。 Instruction Prompting few-shot会花费token，比价昂贵。直接给出指令形式，可能比较经济。understand and follow。be aligned with human intention。\n注意：1. 需要描述任务非常仔细。specific and precise 具体而精准。\n避免说不要做，而说要做。 解释受众对象。 Self-Consistency Sampling Chain-of-Thought (CoT) 按步骤，短句。解决复杂问题。\nTypes of CoT prompts 主要有两种cot：\nfew-shot cot。示例：高质量推理链。cot包含在示例中，数量：4-8. zero-shot cot。 Let\u0026rsquo;s think step by step. Tips and Extensions sefl-consistency sampling 是在decoder层采样多个答案，并且最终经过投票选择最终答案。 prompt具有较高的推理复杂性可以获得更好的性能。cot分隔推理步骤时，使用换行符的效果最好。 使用复杂示例可以提高解决复杂问题的能力，但对简单问题表现不好。 few-shot，示例：使用Question: \u0026amp; Answer: 效果会更好。 在prompt中包含解释的作用可能为小到中等，非事实性的解释会造成错误的答案。 Automatic Prompt Design Augmented Language Models Retrieval Programming Language External APIs Citation Useful Resources References ","title":"Prompt_engineering"},{"content":"测试直接新建md文件 hugo 新建命令：hugo -F --cleanDestinationDir\n1 hugo -F --cleanDestinationDir hugo 新建文档命令：hugo new dir/name.md\n","permalink":"https://tanxiangyuu.github.io/posts/tech/hugo%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","summary":"测试直接新建md文件 hugo 新建命令：hugo -F --cleanDestinationDir\n1 hugo -F --cleanDestinationDir hugo 新建文档命令：hugo new dir/name.md","title":"Hugo命令记录"}]