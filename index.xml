<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tanxiangyu&#39;s log</title>
    <link>https://tanxiangyuu.github.io/</link>
    <description>Recent content on tanxiangyu&#39;s log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 17 Jul 2024 22:33:00 +0800</lastBuildDate>
    <atom:link href="https://tanxiangyuu.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CLINICALGPT: LARGE LANGUAGE MODELS FINETUNED WITH DIVERSE MEDICAL DATA AND COMPREHENSIVE EVALUATION</title>
      <link>https://tanxiangyuu.github.io/posts/paper_read/cinicalgpt/</link>
      <pubDate>Wed, 17 Jul 2024 22:33:00 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/paper_read/cinicalgpt/</guid>
      <description>研究目标 该研究设计和实现了一个能实现多项任务和应用的医学大模型，ClinicalGPT。通过在训练过程中引入多种真实世界数据（如医疗记录、领域特定知识以及多轮对话咨询等）的方式，使其更好地应对多个临床任务。此外，作者还提出了一种全面的评估框架，包括医学知识问答、医学考试、患者咨询以及对电子病历进行诊断分析等多个方面。&#xA;实际问题：&#xA;传统的自然语言处理方法往往需要手动设计特征或规则来进行分类或回答问题，这种方法不仅费时费力，而且难以适应不同场景的需求。而大规模语言模型可以通过无监督学习的方式自动学习到大量的语言规律和语义信息，因此具有更好的泛化能力和适应性。然而，大模型在医疗领域的实际使用中具有局限性。这些模型产生的结果往往与事实不符、逻辑不一致、不连贯，例如引用不存在的文章。它们缺乏真实世界的推理技能和基础，导致了通用和含糊的回复。具体而言，ChatGPT 在医学背景下缺乏深度和洞察力，这可能是由于其基于奖励的训练所使用的排列模型，该模型生成的答案过于笼统，缺乏医学专业知识。&#xA;因此，该研究提出的知识图谱模板化技术和奖励模型可以帮助语言模型更好地理解和应对医学领域中的挑战，从而实现更高效的任务完成。&#xA;是否是新问题：&#xA;虽然在医疗大模型所面临的挑战并非全新，但本研究采用的方法&amp;ndash;开发像 ClinicalGPT 这样的专用模型并针对医疗任务对其进行评估&amp;ndash;却是新颖的。本研究试图直接解决这些已知问题，重点是提高模型在医疗应用中的性能。&#xA;对产业发展的重要意义：&#xA;这项研究对医疗行业的意义在于，它有可能打破特定任务的范式，实现人工智能在医疗保健领域的多功能应用&#xA;新思路与方法 思路&#xA;该研究使用了多种医疗数据集进行模型训练和评估，包括cMedQA2、cMedQA-KG、MD-EHR、MEDQA-MCMLE和MedDialog等。其中：&#xA;cMedQA2是一个包含12万条问题和22.6万条答案的中文医学问答数据集； cMedQA-KG是基于知识图谱构建的医学问答数据集，包含了丰富的医学实体及其关系； MEDQA-MCMLE是基于中国医学考试题库构建的数据集，用于评估临床推理能力； MedDialog是一个由在线平台收集的多轮医学对话数据集，可以模拟医生与患者之间的交互过程； MD-EHR则是一个来自多个大型医院的电子病历数据集，涵盖了各种疾病类型。 为了增强语言模型在特定领域的应用效果，该研究采用了知识图谱模板化技术将知识图谱中的实体和关系转化为适合于指令微调的文本对形式，并利用监督微调的方式进一步优化模型性能。&#xA;此外，该研究还引入了一种奖励模型来为强化学习提供反馈信号，以提高模型生成高质量且有用的输出的概率。&#xA;解决方案关键&#xA;多样的训练数据集 通过知识图谱模板化技术将知识图谱中的实体和关系转化为文本对的形式，可以更好地利用这些结构化的信息来指导模型的学习过程。 同时，奖励模型可以为强化学习提供有效的反馈信号，从而帮助模型生成更加准确和有用的回答。 知识图谱模板化技术如下表所示：&#xA;Prompt Response {疾病}和哪些疾病有关联？ {疾病}可能与{疾病}有关联。 {疾病}可能与其他哪些疾病有关？ {疾病}可能与{疾病}有关联。 {疾病}有哪些常见症状？ {疾病}的常见症状包括{临床表现}。 患有{疾病}的患者可能出现哪些症状？ {疾病}患者可能出现如{临床表现}等症状。 {疾病}的典型临床表现是什么？ {疾病}的典型临床表现为{临床表现}。 患有{疾病}的患者在临床上通常表现为哪些症状？ 患有{疾病}的患者在临床上通常表现为{临床表现}。 诊断{疾病}需要进行哪些检查？ 诊断{疾病}需要进行如{医学检验项目}等检查。 如何检查以确定患有{疾病}？ 确定患有{疾病}需要进行{医学检验项目}等检查。 {药物}主要用于治疗哪些疾病？ {药物}主要用于治疗{疾病}等疾病。 {药物}的适应症是什么？ {药物}的适应症为治疗{疾病}。 如何治疗{疾病}？ 治疗{疾病}的方法包括{医疗程序}。 {疾病}的常见治疗方法有哪些？ {疾病}的常见治疗方法包括{医疗程序}。 {疾病}会引起哪些并发症？ {疾病}会引起{疾病}等并发症。 患有{疾病}的患者可能出现哪些并发症？ 患有{疾病}的患者可能出现{疾病}等并发症。 {药物}与哪些药物存在相互作用？ {药物}与{药物}存在相互作用。 使用{药物}时需要注意哪些药物相互作用？ 使用{药物}时需注意与{药物}的相互作用。 {药物}主要用于治疗哪些症状？ {药物}主要用于治疗{临床表现}等症状。 {药物}的主要治疗作用是什么？ {药物}的主要治疗作用为治疗{临床表现}。 实验设计 本文涉及了以下四个方面的实验：&#xA;医疗对话实验：作者采用了BLEU、ROUGE和GLEU三个评价指标来评估模型生成的对话质量。实验结果表明，ClinicalGPT在所有ROUGE指标上表现优秀，在BLEU-1和大部分ROUGE指标上也表现出色。 医学考试实验：作者选择了MEDQA-MCMLE数据集中的几个类别进行评估，包括医学伦理、呼吸系统、消化系统等。实验结果显示，ClinicalGPT在各个类别中均表现出色，超过了其他LLMs的平均准确率。 诊断实验：作者使用MD-EHR测试集对模型的诊断能力进行了评估。实验结果表明，ClinicalGPT在各个疾病组别中都表现出色，尤其是在消化系统和泌尿系统方面表现最佳。 医学问答实验：作者使用cMedQA2数据集对模型的问答能力进行了评估。实验结果表明，ClinicalGPT在与BLOOM-7B、LLAMA-7B和ChatGLM-6B的比较中表现最好，尤其在与BLOOM-7B和LLAMA-7B的比较中胜出较多。 总的来说，本文的实验结果表明，ClinicalGPT在医疗领域具有出色的应用潜力，特别是在理解和生成医疗对话、医学检查和诊断等方面。然而，在某些特定领域，如妇科和血液系统，可能需要进一步改进。</description>
    </item>
    <item>
      <title>从 huggingface 下载文件到公共服务器</title>
      <link>https://tanxiangyuu.github.io/posts/tech/hf_download/</link>
      <pubDate>Tue, 16 Jul 2024 16:04:34 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/tech/hf_download/</guid>
      <description>怎么下载更快 因为内地特殊原因，从 hf 上下载模式或者数据集速度不堪入目。很多佬都发过提速方法，这里稍微总结一下。&#xA;来源：&#xA;苏洋博客&#xA;如何快速下载huggingface大模型&#xA;huggingface-cli命令下载。&#xA;1 huggingface-cli download microsoft/phi-2 但是非常不建议使用这个默认命令下载，会很慢，而且会下在根目录的缓存文件中，并且使用 git blob 来保存模型文件，会比原本模型更大。&#xA;可以在这个命令基础上，使用和指定一些参数来完成高效下载。 比如：&#xA;1 huggingface-cli download microsoft/phi-2 --local-dir=./models/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download --local-dir 和下载使用的缓存目录 --cache-dir都设置到了当前目录下，开启 --local-dir-use-symlinks=False 可以让下载的文件后续都以非软链文件保存，方便后续保存或者上传服务器。最后，添加--resume-download 可以确保始终接着之前的下载进度继续，如果下载进度有中断。&#xA;HF_TRANSFER使用&#xA;是 hugging face 官方专门为提高下载速度基于 Rust 开发的一个模块，开启后在带宽充足的机器上可以跑到 500MB/s。 但是有非常明显的缺点：&#xA;HF_TRANSFER不可断点重续!!!具体说明文档 太容易断了！ 启用方法：export HF_HUB_ENABLE_HF_TRANSFER=1&#xA;镜像网站使用&#xA;https://hf-mirror.com，设置&#xA;1 export HF_ENDPOINT=&amp;#34;https://hf-mirror.com&amp;#34; 使用摩搭社区下载&#xA;缺点：模型更新有延迟。&#xA;折腾 hf 下载 使用huggingface-cli+HF_TRANSFER下载。&#xA;具体参数：&#xA;--repo-type：下载类型， &amp;lt;model or dataset name&amp;gt;：模型或者数据集名称， --local-dir：本地位置， --cache-dir：缓存位置， --local-dir-use-symlinks：是否使用软链接。 --resume-download：断点续传。 环境设置：&#xA;export HF_ENDPOINT=&amp;quot;https://hf-mirror.com&amp;quot; export HF_HUB_ENABLE_HF_TRANSFER=1 bash 脚本自动配置环境和下载 使用方式示例：</description>
    </item>
    <item>
      <title>Frp</title>
      <link>https://tanxiangyuu.github.io/posts/tech/frp/</link>
      <pubDate>Sat, 13 Jul 2024 16:00:03 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/tech/frp/</guid>
      <description>内网穿透介绍 互联网上两个不同的主机进行通信首先需要知道对方IP。根据IP协议，只有分配了公网IP的设备才能在互联网上通信和传输数据。而中国人口/设备众多，分配到的IPv4资源又少，因此绝大部分情况是通过路由器/交换机转换公网IP后才上网。 位于路由器/交换机后的设备一般是内网设备，分配的IP地址以 192.168/172.16/10.0 开头，属于内网IP。要让内网设备对外提供服务，就需要进行内网穿透。&#xA;frp介绍 frp 是一个开源、简洁易用、高性能的内网穿透和反向代理软件，支持 tcp, udp, http, https等协议。 frp 项目官网是 GitHub - fatedier/frp，中文官方文档地址：frp/README_zh.md。除了安装过程，中文文档对使用过程已经介绍的非常详细，如遇到问题，建议先查看官方文档。 frp工作原理为：&#xA;服务端运行，监听一个主端口，等待客户端的连接； 客户端连接到服务端的主端口，同时告诉服务端要监听的端口和转发类型； 服务端fork新的进程监听客户端指定的端口； 外网用户连接到客户端指定的端口，服务端通过和客户端的连接将数据转发到客户端； 客户端进程再将数据转发到本地服务，从而实现内网对外暴露服务的能力。 frp内网穿透教程 部署frp服务端（外网机器） 在frp下载界面，下载 frp，注意根据机器类型选择版本，这里因为我的客户端有人已经装了 frp_0.33.0_linux_amd64.tar.gz，所以在服务端也下载这个版本。 解压安装包：tar -zxvf frp_0.33.0_linux_amd64.tar.gz 进入解压目录：cd frp_0.33.0_linux_amd64，编辑 frps.ini文件，编辑完之后删除注释！！！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [common] # frp监听的端口，默认是7000，可以改成其他的 bind_port = 7000 # 授权码，请改成更复杂的 token = 12345678 # frp管理后台端口，请按自己需求更改 dashboard_port = 7500 # frp管理后台用户名和密码，请改成自己的 dashboard_user = admin dashboard_pwd = admin enable_prometheus = true # frp日志配置 log_file = /var/log/frps.</description>
    </item>
    <item>
      <title>堆排序</title>
      <link>https://tanxiangyuu.github.io/posts/algo/%E5%A0%86%E6%8E%92%E5%BA%8F/</link>
      <pubDate>Wed, 13 Mar 2024 00:58:19 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/algo/%E5%A0%86%E6%8E%92%E5%BA%8F/</guid>
      <description>对于堆来说，有几个重要的点，虽然简单，但是需要理解，才能理解堆&#xA;完全二叉树 堆是一个完全二叉树 可以分成，大顶堆和小顶堆 大顶堆：父节点大于等于子节点 小顶堆：父节点小于等于子节点 堆的存储：数组形式&amp;ndash;利用完全二叉树的性质。 </description>
    </item>
    <item>
      <title>快速选择排序</title>
      <link>https://tanxiangyuu.github.io/posts/algo/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/</link>
      <pubDate>Mon, 11 Mar 2024 23:54:33 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/algo/%E5%BF%AB%E9%80%9F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/</guid>
      <description>题目：假设有一组无序的数字，找到其中排名第 k 位的数字。&#xA;快排 快速排序是一种优秀的排序算法。&#xA;C++ STL 的 sort，使用的就是“快速排序 + 插入排序 + 堆排序”的方式。&#xA;快排的核心 找准基准值的位置&#xA;通过的是partition操作，将数组分为两部分，小于基准值的放在左边，大于基准值的放在右边。&#xA;然后通过递归，对两边继续进行partition操作。&#xA;问题&#xA;求排名为k的元素，并不需要对整个数组进行排序。 时间复杂度不稳定，最坏的情况会达到O(n^2)。 快速选择排序 针对第一个问题：使用快速选择排序方法解决。在不对数据整体进行排序的前提下，快速找到排名第 k 位的元素，而且时间复杂度还能优化到 O(n)。&#xA;快速选择的核心 当需要快速找到一个元素 X，并且使得小于 X 的元素数量是 k-1 个时，那 X 就是要查找的排名第 k 位的元素了。&#xA;没有必要对整个数组进行排序。&#xA;依旧使用partition操作进行实现。 在partition操作中，将基准值排名ind和 k 进行比较。&#xA;如果 ind 正好等于 k，那说明当前的基准值，就是要找的排名第 k 位的元素； 如果 ind 大于 k，说明排名第 k 位的元素在基准值的前面。接下来，要解决的问题就是，在基准值的前面查找排名第 k 位的元素； 如果 ind 小于 k ，就说明排名第 k 位的元素在基准值的后面，并且，当前包括基准值在内的 ind 个元素，都是小于基准值的元素。那么，问题就转化成了，在基准值的后面查找排名第 k - ind 位的元素。 三种快排的优化 针对第二个问题，稳定系统运行时间。</description>
    </item>
    <item>
      <title>RAG</title>
      <link>https://tanxiangyuu.github.io/posts/llm/rag/</link>
      <pubDate>Mon, 11 Mar 2024 21:15:23 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/llm/rag/</guid>
      <description>RAG-Retrieval Augmented Generation 1. RAG介绍 RAG，检索增强生成技术，是一种基于检索的生成模型，它结合了生成模型的灵活性和检索模型的效率。RAG通过将生成模型与检索模型相结合，实现了高效的文本生成。&#xA;目的：解决LLM的幻觉情况，加深专业领域深度，更新知识库。&#xA;最小RAG的基本结构：&#xA;向量化模块，将文档（文本）片段向量化。 文档加载和切分模块，加载文档和切分文档片段。 数据库模块，存放切分好的文档片段及其对应的向量表示 检索模块，根据query检索相应的文档片段。 大模型模块，根据检索出的文档来回答query 生成answer。 RAG的流程 ：&#xA;索引：将文档库分割成较短的chunk，并且通过编码器构建向量索引。 检索：根据query和chunks的相似度检索相关文档片段。 生成：通过检索到的文档，作为上下文条件，生成answer。 2. 各个模块的code demo code 待补充。。。&#xA;3. 总结 一个最小的RAG包括：&#xA;向量化模块 文档加载和切分模块 数据库 检索模块–向量 大模型模块 4. 论文 [1] RAG: Retrieval Augmented Generation for Dense Text-to-Text Pre-training. </description>
    </item>
    <item>
      <title>PyTorch张量基础教程总结</title>
      <link>https://tanxiangyuu.github.io/posts/llm/pytorch/</link>
      <pubDate>Tue, 27 Feb 2024 21:28:33 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/llm/pytorch/</guid>
      <description>本文是针对PyTorch中的张量（Tensors）的一个基础教程，它详细介绍了张量的定义、特性、以及如何在PyTorch中使用张量进行基本操作。张量是PyTorch中进行科学计算的基础，它们可以视为一个高维数组或矩阵。本教程的主要内容包括：&#xA;张量初始化 从数据直接创建张量：可以直接从数据创建张量，PyTorch会自动推断数据类型。 从NumPy数组创建：可以使用torch.from_numpy()从NumPy数组创建张量。 通过已有的张量创建：可以通过已有的张量来创建新的张量。这种方法会默认重用输入张量的属性（如数据类型），除非显式地进行更改。 使用随机或常数值：torch.rand()创建随机初始化的张量，torch.zeros()和torch.ones()分别创建全0或全1的张量。 张量属性 张量属性：张量的属性包括形状（shape）、数据类型（dtype）和存储的设备（device，如CPU或GPU）。 张量操作 索引和切片：可以使用标准的Python索引和切片操作来访问张量的部分。 张量重塑：reshape可以改变张量的形状而不改变其数据。 张量合并：torch.cat可以用来在给定维度上合并张量序列。 张量乘法：介绍了元素乘法（*或torch.mul）和矩阵乘法（@或torch.matmul）。 张量与NumPy之间的转换 张量可以很容易地与NumPy数组相互转换，使用numpy()方法从张量转换为NumPy数组，使用torch.from_numpy()从NumPy数组转换为张量。这两种类型的转换是共享底层内存的，因此修改其中一个会影响另一个。 自动微分 自动微分：PyTorch中的自动微分是通过autograd模块实现的，它提供了张量上所有操作的自动微分。这对于深度学习训练中的梯度计算非常有用。 运行在GPU上 张量可以被移到GPU上：使用.to方法可以将张量移动到任何设备上，这对于加速计算非常重要。 本教程适合初学者，通过详细的示例和解释，帮助读者理解和掌握如何在PyTorch中有效地使用张量进行数据操作和计算。</description>
    </item>
    <item>
      <title>如何向黑客有效地提问</title>
      <link>https://tanxiangyuu.github.io/posts/tech/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/</link>
      <pubDate>Tue, 27 Feb 2024 20:28:33 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/tech/%E6%8F%90%E9%97%AE%E7%9A%84%E6%99%BA%E6%85%A7/</guid>
      <description>原文链接&#xA;引言 尊重与期待：黑客欣赏高质量问题，对好问题持积极态度，但对不假思索或懒惰提问者有所抵触。 提问前的准备工作 自行寻找答案 搜索论坛帖子 使用搜索引擎查询 阅读手册与常见问题解答(FAQ) 自行尝试和测试 向身边专家咨询 如是程序员，阅读源代码 展示前期努力：在提问时，告知你已经尝试过的解决途径，证明你不是一个坐享其成者。 提问技巧 选择合适的提问场所：避免在不相关的论坛提问，查找并选用专门针对你问题的主题论坛或邮件列表。 利用Stack Overflow和Stack Exchange：在提问前先搜索，然后在对应主题的Stack Exchange网站上提问。 邮件列表与IRC：参加项目邮件列表，若有必要，可在开发者邮件列表提问，前提是你已经尝试过用户列表。 编写有效的标题： 标题示例：目标——差异式描述 示例：X.org 6.8.1鼠标指针在MV1005显卡芯片组下的变形问题 方便回复：不要请求直接回复到个人邮箱，而是使用恰当的论坛或邮件列表提问，并确保标题能准确反映问题内容。 清晰表述： 使用正确、清晰、准确的语言，避免拼写、语法错误。 尊重非母语使用者，但力求表达清晰，必要时声明语言障碍。 便于阅读的格式： 使用纯文本格式，避免HTML和特殊格式。 不要发送封闭格式的文档，如Word或Excel文件。 在论坛中适度使用表情符号和格式，保持专业简洁。 问题描述详尽： 描述问题的具体表现。 提供环境信息，如操作系统、应用版本等。 描述已经进行的研究与诊断步骤。 提供可能导致问题变化的背景信息。 提供重现问题的简明步骤。 精炼问题：提供简明扼要且具有针对性的信息，尽量缩小问题范围。 谨慎声称发现Bug： 在确定是Bug之前，先假设可能自身操作不当。 提供详尽的重现步骤及可能的补丁或回归测试证据。 低声下气无助于解决问题 避免以过分谦卑或模糊的方式提出问题，应清晰表述背景及问题细节，不必自我贬低。明确问题的具体症状而非主观猜测，以便他人准确判断和诊断。 提供精确的技术信息 当提问关于技术问题时，务必提供详尽的硬件配置、软件环境及错误现象等信息，如编译错误的例子所示，确保对方能看到与你相同的现象，而非仅听信个人揣测。 遵循问题发生的时间顺序 记录问题出现前的操作流程，按时间顺序详细描述，包括系统的反应和出现问题的具体环节。如有必要，提供调试日志和相关设置，确保信息具有针对性和实用性。 聚焦问题实质而非操作过程 提问时首先阐明你的最终目标，而非纠结于某一无效步骤。例如，当询问颜色选择器获取RGB值时，应说明实际需要达成的任务，以便得到更有效的解决方案。 鼓励公开交流 避免请求私下回复，提倡在公共平台讨论问题，以便更多人参与、纠正错误，并为解答者带来社区认可。只有在特殊情况下，如预期收到大量重复回复时，可提议私下整合信息后回馈至公共平台。 清晰简洁地表述需求 明确告知需要何种形式的帮助，限制所需投入的时间和精力，使专家更容易给出有针对性的回答。简化问题表述，比如求教参考资料来源优于直接要求解释概念。 提供最小化复现代码 当涉及编程问题时，附上精简且能够复现问题的代码片段，明确指出期望结果和实际结果之间的差异。创建最精简的测试用例有助于他人快速定位问题，从而提高获得有效回复的可能性。 对待家庭作业的态度 对于疑似家庭作业性质的问题，黑客社群鼓励自行解决以积累经验。可以请求提示，但不应索要完整答案。在适当场合如用户论坛中提问时，表明你已尽力但仍需指导，或许能得到有益的提示。 去除无意义提问以提升问题质量 避免使用无助于解决问题的结尾疑问句，如&amp;quot;有人能帮我吗？&amp;quot; 不明确的问题加上这类问句反而显多余，易遭黑客社区反感 尽量减少&amp;quot;是/否&amp;quot;型提问，除非期望得到确切的二元答复 慎用“紧急”标签以获取有效关注 标注“紧急”往往适得其反，可能导致问题被忽视或删除 在非专业场合高调标榜紧急可能因垃圾信息过滤而失效 特殊情况下的紧急提及须礼貌且基于共同兴趣点 礼仪助力沟通，提升获得解答的可能性 表达感谢，如使用“请”和“谢谢您的关注”，展示对他人力气的认可 礼貌虽非首要，却有益于问题得到关注，尤其在连续提问情境下 先致谢后再次感谢回复者的具体帮助，避免误解 问题解决后的跟进与分享 解决问题后及时告知并感谢参与者 使用“已解决”等标记更新原话题标题，方便他人查阅 简洁概述问题解决方法及原因，提及关键协助者 提供简洁的调试摘要或指出避免问题的途径 正确反馈问题解决状态有助于满足解答者成就感 考虑编写文档或添加FAQ，防止他人重复遇到相同问题 如何解读答案 RTFM 和 STFW：遭遇经典回应的意义与应对 当收到RTFM或STFW回复时，这意味着你应该查阅相关手册或自行上网搜索。 这类回答表明信息易于获取，自行搜索能促进学习，而非直接给予答案。 收到此类回复应视为对方已在某种程度上提供了关注，对此表示感谢。 面对困惑时的正确求解姿态 若无法理解回复，尝试自行解决，参考手册、FAQ、网络资源或请教他人。 提出疑问时展现自主探索过程，如举例询问具体的细节而非基础概念。 应对无礼回应与黑客文化 黑客圈内的直率交流可能被误认为无礼，遇此情况保持冷静。 如确实遭受冒犯，社群内其他成员可能会介入纠正不当行为。 对于真正的冒犯者，有力反击可被接受，但新手需谨慎判断，避免陷入无谓争执。 黑客文化中的某些特质可能源于独特的社交习惯，不必过分解读。 Jeff Bigler 的观点：应对策略 Jeff Bigler 关于“社交过滤器”的观察可作为理解和适应黑客社群互动方式的参考。 如何避免扮演失败者 面对批评与纠正 接受并忍受在黑客社区中因失误而受到的公开批评，这是社区标准得以维持的方式。不应期待所有意见都通过私人途径传达，避免将建设性批评视为个人攻击。 处理挑衅行为 遇到无端指责时，保持冷静，避免陷入口水战。黑客指出错误是出于关心社区和个人成长，不应为此抱怨或要求特殊对待。 学会区分口水战与实质性回复 多数口水战无需理会，确认其中是否包含对问题实质的解答或有价值的建议。 不该问的问题及其典型回应 寻找资源路径 利用搜索引擎寻找所需程序或资源，基本查询能力应当具备。 操作方法问题 提问过于宽泛，未能明确问题焦点，建议先自行研究Y问题的本质。 配置问题 自行阅读手册（RTFM），自行查找答案。 文件转换问题 自行尝试并验证可行性。 无效的程序/设置问题表述 提供具体问题详情，避免空泛陈述。 针对Windows的问题 优先考虑更换操作系统或在适当场合提问，如涉及与开源软件交互问题。 质疑系统工具有效性 确保问题归因准确，提供详细证据。 安装Linux或其他操作系统问题 寻求本地用户群组的帮助，提供具体故障细节。 非法活动请求 黑客不会支持非法行为，此类问题不会得到回答。 好问题与蠢问题的区别 展示搜索努力 聪明的问题表明提问者已经尽力搜索但仍未解决问题，寻求进一步指引。 尊重他人时间 避免责备他人或表现出傲慢态度，详细描述问题背景和已尝试的解决方案。 得不到回答时的对策 保持耐心 问题可能因多种原因暂时未得到回答，重复张贴问题非明智之举。 寻求其他援助渠道 加入用户群组或寻求商业支持，理解免费技术支持的局限性。 如何更好地回答问题 友好态度 保持礼貌和善，理解提问者可能承受的压力。 私下回复初犯者 对真诚的新手私下指导，避免公开羞辱。 不确定时明确表态 避免给出错误答案，鼓励提问者提供更多细节。 避免误导性玩笑 不要给出可能破坏提问者设置的玩笑性建议。 引导提问者细化问题 通过反问引导提问者提供更多细节，促进双方学习。 给出高质量答案 避免给出权宜之计，推荐更好的工具或重新定义问题。 正面回答并分享技巧 在回答时强调解决问题的方法，而非单纯提供结果。 推动社区进步 从问题中总结经验教训，改进文档和常见问题解答，以便未来参考。 </description>
    </item>
    <item>
      <title>Prompt_engineering</title>
      <link>https://tanxiangyuu.github.io/posts/llm/prompt_engineering/</link>
      <pubDate>Tue, 20 Feb 2024 18:48:33 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/llm/prompt_engineering/</guid>
      <description>Lilian Weng&amp;rsquo;s blog&#xA;提示工程，也被人说为是上下文学习。它的本质上是用来对齐和激活大模型的能力。它需要大量的实验和启发式方法。&#xA;她提到，迭代的prompt 和 外部工具的使用 没有那么容易被接受。&#xA;Basic Prompting Zero-Shot 就是向LLM简单的提问，向人类交流一样。&#xA;Few-shot 少样本提示包括完整的输入和输出示例，以便大模型理解问题，一般具有更加优秀的回答，能够发挥大模型的能力。但是加入示例会消耗一部分token。&#xA;prompt的格式选择、训练样本、训练样本的顺序都会对结果造成很大的影响。&#xA;有研究调查说明几个有趣的现象：1. 训练数据的label如果分布不均衡，会极大的影响模型能力。2. 最近偏差现象，模型可能会返回训练的最后的几个重复标签，最后训练标签可能权重较大。3. 大模型更加倾向产生常见的token。&#xA;Tips for Example Selection 样例选择tips：&#xA;选择一些语义上相似的样例。在embedding层使用k-nn聚类。 为了选择多样的回答，可以使用有向图，通过相邻节点选择数来打分，如果相邻节点选择的多，则打分低，选择几率下降。节点连接通过节点间的embedding cosine相似度判断。 多次采样试验中找出分歧或熵较大的示例。然后对这些示例进行注释，以用于少量提示。 Tips for Example Ordering 选择示例多样性，并且随机排列 上下文示例排列不同，结果不同。增大模型规模或者包含更多样例不能消除这种差距。 Instruction Prompting few-shot会花费token，比价昂贵。直接给出指令形式，可能比较经济。understand and follow。be aligned with human intention。&#xA;注意：1. 需要描述任务非常仔细。specific and precise 具体而精准。&#xA;避免说不要做，而说要做。 解释受众对象。 Self-Consistency Sampling Chain-of-Thought (CoT) 按步骤，短句。解决复杂问题。&#xA;Types of CoT prompts 主要有两种cot：&#xA;few-shot cot。示例：高质量推理链。cot包含在示例中，数量：4-8. zero-shot cot。 Let&amp;rsquo;s think step by step. Tips and Extensions sefl-consistency sampling 是在decoder层采样多个答案，并且最终经过投票选择最终答案。 prompt具有较高的推理复杂性可以获得更好的性能。cot分隔推理步骤时，使用换行符的效果最好。 使用复杂示例可以提高解决复杂问题的能力，但对简单问题表现不好。 few-shot，示例：使用Question: &amp;amp; Answer: 效果会更好。 在prompt中包含解释的作用可能为小到中等，非事实性的解释会造成错误的答案。 Automatic Prompt Design Augmented Language Models Retrieval Programming Language External APIs Citation Useful Resources References </description>
    </item>
    <item>
      <title>Hugo命令记录</title>
      <link>https://tanxiangyuu.github.io/posts/tech/hugo%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Mon, 22 Jan 2024 00:05:03 +0800</pubDate>
      <guid>https://tanxiangyuu.github.io/posts/tech/hugo%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/</guid>
      <description>测试直接新建md文件 hugo 新建命令：hugo -F --cleanDestinationDir&#xA;1 hugo -F --cleanDestinationDir hugo 新建文档命令：hugo new dir/name.md</description>
    </item>
    <item>
      <title>Archive</title>
      <link>https://tanxiangyuu.github.io/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://tanxiangyuu.github.io/archives/</guid>
      <description>archives</description>
    </item>
    <item>
      <title>Search</title>
      <link>https://tanxiangyuu.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://tanxiangyuu.github.io/search/</guid>
      <description>search</description>
    </item>
  </channel>
</rss>
